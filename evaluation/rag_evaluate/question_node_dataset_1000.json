{
    "queries": {
        "07360df9-c77f-4f28-808b-2852449492e2": "\"Как работает алгоритм градиентного бустинга в анализе данных?\"",
        "6a5c7c08-3000-4a7a-a629-d430c52c667f": "\"Какие основные компетенции должен иметь аналитик данных?\"",
        "3d25297e-1d02-4d70-ab88-acb8b97fc132": "\"Что такое атрибуты и методы в Pandas DataFrame?\"",
        "974dd1c6-85d9-4dbc-9f5d-d940b970a48a": "\"Как дерево решений выбирает вопрос для прогнозирования данных?\"",
        "9dcec08f-b484-44c3-a660-abb0853476ae": "Какие навыки должен иметь аналитик данных согласно тексту?",
        "037ce451-c7f2-406a-bff8-b2a5f70c6fc4": "\"Какие изменения были внесены в запрос для анализа данных?\"",
        "dafd92ee-bf73-474f-be16-dc26bdf58bf1": "Какие хардскиллы важны для аналитика данных в сфере программирования?",
        "69d9c753-a89c-4755-871d-873ed358291b": "Какие инструменты аналитики рекомендуются для работы с данными?",
        "2ccbd314-7806-4f64-89f9-27632d2ae583": "Как работает метод изоляционного леса для обнаружения выбросов?",
        "82580531-4b57-41c9-b433-2fa2d45091ea": "\"Какие навыки должен иметь аналитик данных согласно тексту?\""
    },
    "corpus": {
        "927cd889-78dd-4741-b234-8275a79e3779": "Мы выбрали первый вопрос. Что же делать дальше? Ответ очень простой. То же самое. Мы повторяем процесс выбора оптимального вопроса аналогично первому. При этом выбираем уже только для значений, для которых выполняются условия предыдущих вопросов. Продолжаем, пока не кончится наблюдение, либо если мы хотим искусственно ограничить глубину дерева. Про это дальше. Остается вопрос. Какие прогнозы давать дереву, исходя из ответов на вопросы? Прогноз дерева – это не что иное, как среднее значение цены для всех наблюдений из исторических данных, для которых выполняются условия. Таким образом, для домов с похожими ответами на вопросы дерево дает похожие прогнозы. Вот, например, для маленьких домов дерево прогнозирует с помощью среднего для таких же домов. Соответствующим образом и для больших. Итак, мы поняли, как деревья обучаются на данных, как они выбирают наилучшие вопросы и даже как делают прогнозы. Переходим к градиентному бустингу. Сначала градиентный бустинг просто обучает первое дерево без каких-то изменений. Первое дерево полностью идентично тому, что мы только что разобрали. Затем алгоритм бустинга использует первое дерево, чтобы сделать прогнозы на исходном наборе данных. При этом бустинг искусственно ограничивает глубину дерева, поэтому прогноз отличается от истинных значений. Вычитая из истинных значений наш прогноз, мы получаем ошибку. И уже следующее дерево алгоритм обучает прогнозировать ошибку предыдущего. Таким образом, все то, что не получилось у первого дерева, возможно получится у второго. По крайней мере, оно попробует исправить ошибки первого дерева. Два дерева, конечно, лучше, чем одно, но и они несовершенны. Мы опять получаем ошибку, вычитая из реальной цены прогнозы первого и второго деревьев. Процесс повторяется, и каждое новое дерево обучается на ошибках предыдущих. И таким образом, с каждым новым деревом ошибка снижается. Как правило, для наилучшей точности тренируют 100 и более деревьев. На этом все. Сегодня мы изучили алгоритм градиентного бустинга.",
        "d13c946f-fa35-474c-8012-216557272c04": "Привет, меня зовут Саша, и сегодня мы с вами поговорим, кто такой аналитик данных. Так кто же такой дата аналитик? Это тот человек, который отвечает на вопросы бизнеса, основываясь на данных. При этом аналитики, работающие в разных компаниях или над разными доменными областями, могут иметь совершенно разные инструменты. Однако существует некий базовый набор компетенций, которыми должен обладать хороший аналитик данных. Это Python, SQL, умение визуализировать данные, работать с биосистемами, а также уметь строить АБ-тесты. Сегодня я как раз и расскажу, почему именно этот набор компетенций — это хороший буст для старта в любую область дата-сайенса. Начнем разговор мы с математической базы. Дата аналитик — это как раз одна из тех специальностей, которые требуют хорошего математического бэкграунда, в отличие, например, от дата инженерии. Математических скиллов, освоенных на позиции дата-аналитика, вам хватит для того, чтобы потом освоить машинное обучение или уйти в любую другую область дата-сайенса. Теперь давайте поговорим о Питоне, потому что каждый уважающий себя дата-аналитик должен уметь пользоваться этим языком. Понимать, как подключиться к базам данных, как работать с данными, их визуализировать. Также разбираться в классическом DSTech — это SciPy, Pandas, NumPy, Secular. Также этот язык пригодится не только в области Data Science, если вы захотите дальше развиваться в области разработки, он будет просто неизменим. Потому что Питон — это не только о работе с данными, однако вам, скорее всего, придется подучить еще и ООП. Теперь пришло время поговорить об SQL и базах данных. Как правило, любой дата-аналитик всегда работает хотя бы с одним средством управления базами данных, будь то Postgres или Clickhouse. Также базовые знания SQL пригодятся в любой области. Да, синтакси с SQL может отличаться, однако если вы знаете, как работать с одной системой, можете быстро перестроиться на работу с другой базой данных. А также умение составлять сложные запросы, пользоваться различными видами джойнов, кодными функциями, поможет вам в дальнейшем в переходе в дата-инженерию. Еще хочется сказать несколько слов о GTA и работе с командной строкой. Да, возможно, это не самые важные навыки в работе дата-аналитика, однако они полезны в любой сфере IT.",
        "1de2b2c9-8f97-4ecd-8e44-64effb460e4e": "Pandas DataFrame — это не только табличка, в которой хранятся данные, в этом смысле он бы не сильно отличался от Excel или CSV. В этом смысле правильно кто-то из вас написал, что Pandas — это просто как Excel, в котором можно только в Python работать с большими данными. Но это еще не просто табличка, в которой лежат данные, но это еще возможность этой табличкой много чего делать и много чего про нее узнавать. Например, если мы напишем табличка с данными Shape, мы посмотрим, сколько в ней лежит данных. Если мы напишем табличка с данными DataTypes, мы посмотрим, какие типы данных в нашем DataFrame лежат. И вот такие вот штуки называются атрибуты DataFrame. То есть у DataFrame, когда мы считали эти данные, есть очень много информации, которую мы про него знаем. Если вы зайдете просто в... Соответственно, здесь вот... Если мы просто зайдем в саму документацию Pandas, и здесь, например, напишем «Attributes». Обратите внимание, что у Pandas DataFrame... Сейчас мы его найдем. Еще раз прошу прощения. Так... У Pandas DataFrame есть довольно много... Вот, видите, атрибутов. Это то, к чему можно просто обратиться через точку и узнать что-то полезное про этот DataFrame. Например, мы можем посмотреть... Смотрите, DataFrame.columns возвращает нам список колонок. То есть мы могли бы написать «retail.columns» и, соответственно, просто вернуть название колонок. Вот атрибуты — это то, что хранится, какая полезная информация у нас есть про DataFrame. И помимо атрибутов у DataFrame есть методы. И вот методы мы действительно записываем немного иначе. Мы вызываем именно через скобочку. То есть атрибуты — это то, что можно как бы узнать, по сути, сходить на полочку под нужным названием. А методы — это то, что позволяет нам какую-то сделать преобразование. И, по сути, весь анализ данных сводится к тому в Pandas, что мы комбинируем использование различных методов и атрибутов для того, чтобы отвечать на различные аналитические вопросы. Вот этот вот «retail.tail» — по сути, метод, который позволяет нам вернуть последние пять строчек DataFrame. Ну, кажется, очень простой метод и ничего необычного. Однако методы бывают более сложными. Например, сгруппировать какую-то хитрую величину, посчитать скользящее среднее, отсортировать по какой-то переменной и вернуть все это дело в нужный DataFrame. Поэтому, соответственно, сегодня с более сложными методами мы и поработаем. Ну что ж, соответственно, это небольшое введение в то, что такое Pandas.",
        "f71cbf17-64bb-49ea-9eb8-c51201e7f6ff": "И градиентный бустинг, в свою очередь, комбинирует множество деревьев решений, чтобы улучшить точность прогнозирования. При этом, каждое новое дерево решений – это самостоятельный алгоритм машинного обучения, которому нужны все те же данные, что и сильному алгоритму. Более того, каждое дерево способно давать прогноз самостоятельно. Поэтому для понимания полной картины градиентного бустинга необходимо понять, как работают деревья решений. Именно поэтому сначала мы разберем механизм работы деревьев и только потом перейдем к градиентному бустингу. Вот так, например, выглядит уже обученное на данных дерево решений. Как вы можете видеть, оно подбирает соответствующую цену, исходя из значений измеряемых характеристик дома. Например, в этом случае оно первым делом спрашивает, а размер дома меньше или равен 200 футам? В нашем случае ответ «да», поэтому мы переходим в соответствующую ветвь. Обратите внимание, что второй вопрос зависит от нашего ответа на первый. Дальше мы снова отвечаем «да», потому что количество комнат в нашем доме меньше пяти. Таким образом, исходя из наших ответов, обученное дерево выдает подходящую цену для дома. Обратите внимание, что прогноз меняется в зависимости от ответа. Непонятно одно, откуда дерево знает, какие вопросы задавать и, соответственно, какие прогнозы делать, исходя из ответов. Так вот, дерево выбирает подходящий вопрос, исходя из качества разделения исторических данных. Оно анализирует все возможные варианты разделения данных на две группы и выбирает из этих вариантов наилучший. Чтобы выбрать лучший из всех возможных вариантов, дерево применяет формулу MSE от английского mean squared error, что и в переводе на русский означает средняя квадратичная ошибка. MSE помогает нам оценить качество прогнозирования числовой величины с помощью среднего значения. Применительно к ценам домов формула отвечает на вопрос, если бы мы прогнозировали, используя среднюю цену, насколько сильно бы мы ошибались в таком случае. Чтобы посчитать MSE, мы делаем очень простую вещь. Вычитаем из каждой цены в нашей выборке среднюю, а затем возводим в квадрат и усредняем. Усредняем, по-другому складываем, суммируем и делим на количество наблюдений.",
        "a158b488-05d0-448d-9029-f94cda0ee615": "И в этом-то и мой панч, что в хорошо настроенном аналитическом слое аналитик не должен знать SQL. Если аналитик пишет SQL-запрос, что-то пошло не так. Я понял о чем ты. Какой SQL? Может еще код на PHP написать, чтобы что-нибудь выгрузить? Рендерить модуль. Да, вот, может как бы еще. То есть если данные настроены правильно и удобно, если все витринки сджонены как нужно, если есть хорошие исторические слои, то все данные можно просто в табло смотреть, калочки нажимать. В хорошем смысле этого слова. Когда ты пошел писать SQL или в Pandas и крутишь данные, мне кажется, сразу же все скажут, ну, это такая сложная задача, этот данный ресерчер. Но кажется, на практике это происходит просто потому, что данные плохо хранятся. И приходится на очень банальные вопросы отвечать при помощи SQL-запросов. Ну, да, да. Но это же с другой стороны. Такой человек, он же спаситель. Да, и я сейчас читаю вакансии аналитиков, и мне кажется, прямо вот мне иногда хочется в этот чат ворваться, начать спорить с людьми. Написано, мы ищем аналитика, и дальше идет перечисление вещей. Собрать в единое хранилище данные из разных источников. Это ДВХ. Настроить какие-то пайплайны, проводить A-B-тесты и понимать инсайты в данных. Вы уже определитесь, вам нужен дата-инженер или аналитик. Ну, да. И, ну, жизнь суровая штука. Мне кажется, действительно, очень много аналитиков такие мамкины дата-инженеры, которые и в Hadoop сходят, и там в Клейхаусе что-то понастраивают. Но мой большой прогноз, что очень сильно сейчас будет расти запрос на вот таких вот чуваков, которые знают, как работает аналитика и зачем это нужно, но с очень сильными хардскиллами. Ну, дата-инженеры. В общем, да. Дата-инженеры с очень хорошим пониманием аналитических задач. Что-то такое. В моем мире сложилось немного, что дата-инженеры оторваны от бизнеса, и они просто как бы сидят и данные перекладывают из витрины в витрины. Ну, да. И в хорошем смысле. Я вообще обожаю дата-инжиниринг и сам обожаю из витрины перекладывать.",
        "2d299975-4df5-4d5e-8f49-8bf0ff58cbc5": "И сейчас у нас ретеншон второго дня, он расположен как бы по диагонали, что, как вы понимаете, не самое удачное восприятие, удачный угол для восприятия. Поэтому можем немножечко изменить еще наш запрос и добавить еще одну переменную, которая просто будет показывать, сколько дней прошло с момента, получается, даты активности и стартовой даты. Давайте назовем это просто как-нибудь вот так вот, dtn. Проверим, что все работает и посмотрим на результат. Вот здесь посмотрите, у нас теперь получается между датой и стартовой датой, когда они совпадают, это 0 дней прошло, и как бы между 24 и 25 числом, это у нас будет 1 пройденный день. Это нам подходит, поэтому мы идем в наш pivot table, здесь мы, получается, делаем тогда edit visualization, и я думаю, что мы можем, знаете что, вообще убрать, например, из верхней строчки месяц и dt конкретные числа и добавить вот такое число, именно просто количество пройденных дней для этой когорты. Сохраним этот график и давайте теперь посмотрим на него на дашборде. Во-первых, здесь у нас сейчас все обновится, прошу прощения, когорт retention, тут мы вроде бы все сохранили, возможно, мы не сохранили просто сам запрос. Вот так вот. Здесь обновим наш дашбордик, сделаем его refresh, чтобы он подтянул. И посмотрите, насколько, во-первых, стал более удобен сам график для восприятия, и во-вторых, насколько теперь у нас появилась интересная возможность по колоночкам у нас теперь количество дней, которые прошли с момента первой активности. 0 дней – это просто 100%, это наш первый день наших пользователей, все 100% пользователей из этой когорты активны. А дальше мы смотрим для каждой когорты, какой процент пользователей был активен. На первый день после запуска, на второй день, на третий, на четвертый, на пятый. И такой формат очень удобен, потому что…\nМашинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение.",
        "f5fce271-577f-4ee0-b680-a8d434aabbc9": "Поэтому хардскиллы — довольно важный акцент, и Airflow, пожалуй, закрывает такой первый блок хардскиллов. Airflow — это некоторый оркестратор, когда вам нужно написанные вами скрипты на питоне заставить работать в такой стройной, единой системе. Представьте типичный день аналитика, вы приходите на работу, и у вас есть скрипт, который сначала выгружает данные из хранилища, потом как-то их хитро предобрабатывает, потом высчитывает какие-то различные бизнес-метрики, а потом, допустим, отправляет эти данные на визуализацию на дашборде. По сути, это такая череда событий, которые должны друг за другом произойти, и в этом смысле Airflow — это как раз такой инструмент, который позволяет явно указывать, какие скрипты выполняются вслед за какими. Это тоже довольно очень важный хардскилл, который мы специально добавили в нашу программу. Очень часто Airflow можно встретить скорее в программах, которые посвящены обучению, не знаю, дата-инженеров, но мое глубокое убеждение, как я уже вам сказал, что аналитик должен обладать довольно хорошими хардскиллами, и в огромном количестве компаний, если вы понимаете какие-то базовые вещи, работы с кодом, с Airflow, с Git, вы на самом деле очень сильно открываете себе карьерные перспективы, потому что такие аналитики действительно ценятся на рынке. То есть очень важна комбинация и хардскиллов, и каких-то пониманий о бизнес-процессах, и софтскиллов, но об этом чуть дальше. Поэтому вот Airflow — это такой заключительный, так скажем, финальный модуль, посвященный программированию, который научит вас выстраивать уже некоторые такие, как это принято звать, пайплайны по работе с данными и оркестрировать уже довольно сложные задачи по предобработке и каких-то хитрых манипуляций с данными при помощи питона. Закончив с программированием питона, мы переходим к блоку, который называется SQL. SQL — это на самом деле тоже джентльменский набор, без этого никуда. Как я сказал, что если в 90% случаев в резюме от вас будут ожидать, что вы знаете питон, то вот в 100% случаев от вас будут ожидать, что вы знаете SQL. База данных. База данных — это основной способ хранить данные в индустрии. И на самом деле есть огромное количество различных баз данных. Есть там, вы слышали, наверное, MySQL, Postgres, Clickhouse, Vertica.",
        "92d469ee-6e97-4eb8-b37b-bc153a4f4aef": "При этом сразу же оговорюсь, что мы не будем уделять много времени именно тому питону, который нужен условно, я не знаю, разработчику, а мы сразу сделаем большее погружение именно в питон как инструмент аналитики. То есть поработаем с различными библиотеками, Pandas, Seaborn, NumPy, которые нужны именно аналитику. Поэтому питон — это такой просто must-have модуль, который позволяет вам овладеть, пожалуй, главным инструментом, который есть у аналитика для различных предобработок данных, поиска каких-то закономерностей, работы с данными довольно большого объема, которые уже просто не помещаются в Excel и требуют уже обработки при помощи языка питон. Поэтому питон очень полезный, важный и самый первый открывающий нашу специализацию модуль, который буду читать вам я и мой коллега Саша Ильин. Второй модуль называется «Введение в ГИД», и пока что мы все еще остаемся в зоне работы с программированием, с языками программирования. И есть такая шутка, чем отличается код, который написал аналитик от кода, который написал разработчик. Ответ, что код аналитика должен сработать хотя бы один раз, а дальше и так сойдет. В этой шутке есть доля правды, потому что от аналитика обычно не требует такого, знаете, production-ready кода, который пишут хардкорные разработчики, девелоперы. От аналитика скорее требуется, как я уже сказал, анонсируя наш первый модуль, чтобы вы использовали питон как инструмент для предобработки и анализа данных. Но при этом писать правильный и хороший код, это все равно очень важно, потому что правильный и хороший код — это на самом деле гарант того, что вы решили вашу аналитическую задачу без ошибок. Если вы пишете код, который тяжело поддерживать, который вы сами не понимаете и который не понимают ваши коллеги, то с очень большой вероятностью вы просто допускаете ошибки при работе с данными. И для того, чтобы научиться писать код, который можно шерить с вашими коллегами, совместно работая над какими-то проектами, очень часто в индустрии используется система версии контроля кода, ГИД и различные инструменты для хранения и коллаборации внутри компании — GitLab, GitHub. Это такой условно Google-документ для разработчиков и аналитиков, где можно совместно писать код и им делиться.",
        "7e9bbfc5-d7ac-489e-8d36-5685749fcb10": "Большинство алгоритмов работает так. Давайте посмотрим на наши данные, посмотрим, где у нас сконцентрировано все основное, а вот все, что за пределами основного, обзовем выбросами. Методы, которые будем разбирать мы, называются методами изоляционного леса или isolation forest. В классическом его варианте или в улучшенном модифицированном варианте, что в английском названии будет звучать как extended isolation forest. Давайте разбирать, что здесь происходит. Я думаю, что много кому известен алгоритм случайного леса. Это штука, которая строит деревья решений, в каждом дереве делается прогноз, при этом каждое дерево строится по подвыборке наших признаков и по подвыборке с возвращением из наших данных. И за счет этого при голосовании деревьев, которые будут построены, мы получаем как бы более опасный ответ. Здесь идея очень похожая, но только наша задача построить не предсказание какой-то известной переменной, потому что у нас нет готового класса, мы не умеем определять, что есть выброс, а что не есть выброс. Мы должны построить дерево, которое каким-то образом распилит наш датасет в таком виде, чтобы по этой распиленной структуре мы могли понять, где выброс находится. Идея следующая. Isolation Forest делает примерно так. Берется ваш набор данных и на нем начинается построение нескольких деревьев. Берем первое дерево, строить будем так. Выберем случайную подвыборку наших данных. Берется подвыборка не с возвращением, просто подвыборка данных. Причем подвыборка берется наперед заданного размера, это гиперпараметр вашего алгоритма. Вы выбрали какую-то подвыборку ваших данных, после чего на подвыборке этих данных вы случайным образом выбираете признак, а потом случайным образом в области определения на вашей выборке этого признака берете случайное разбиение. Например, у вас был набор данных с тв-рекламой и там тв-реклама, вложение в тв-рекламу.\nот нуля до двух тысяч. Вот мы две тысячи добавили и испортили наш датасет. Вот в этом промежутке из равномерного распределения генерируется какое-то число. Дальше делается следующее. Мы проверяем значение в текущем наблюдении больше или меньше этого числа. Если меньше, отправляем налево. Если больше, отправляем направо. Меньше или равно и больше, чтобы там точно все было. Так наш датасет делится на две части.",
        "15071151-8a29-4c6a-a652-a5e117c8e1f0": "Мне кажется, отчасти вот эта стадия заставляла аналитиков так идти в hard скилы, что если ты там Python и SQL не знаешь, то какой-то вообще аналитик. Что само по себе на самом деле звучит странно. Ты же не программист, конца-то концов. Вот сейчас более-менее кажется, мы выходим на тот период, когда с этим будет покончено, и всякие штуки из разрядов данных, DWH и ETL уже окончательно перейдут в зону ответственности дата инженеров. Что это изменит на рынке? Вот сейчас есть такая заклинание. Вот аналитик – это человек, который знает SQL и Python. Правильно я понимаю, что исходя из того, что мы только что обсудили, это как раз в сторону... Это commodity. Сейчас аналитик – это человек, который знает Python и SQL только потому, что это позволяет ему быстро решать какие-то задачи. Ты можешь попытаться без знания SQL и Python собрать данные из Яндекс.Метрики или, не знаю, там, данные из рекламных кабинетов, еще откуда-то. Просто Python'ом это сделать проще. Нет, ну вопрос кажется, что по-хорошему это просто должны заниматься другие люди. Можно заниматься командой ДВК, ETL. Пусть настраивают эти пайплайны. Опять же, мой личный пример. Работал я в Rambler. И когда у меня там сложный запрос к базе, я шел к разработчику и говорил, Олег, мне нужно достать вот это, вот это, вот это. Мне это нужно регулярно. Олег, разработчик, сделай мне, пожалуйста, код, который будет масштабируемый, который можно, не знаю, прочитать, который будет работать. Я напишу код на коленке, который там позволит мне вытащить там одни, вторые, третьи, пятые данные. И при этом я говорю, вот ты напишешь нормальный код, расскажешь мне, как он работает, и я буду его там, не знаю, я буду оператором этого скрипта. И при этом это правильное разделение. Вот возвращаясь к вопросу. Например, маркетолог в принципе тоже может знать там Python и SQL, если это позволяет ему, например, в какой-то степени читать скрипты и становиться оператором этих скриптов и пытаться быстрее подходить к выполнению разного рода задач."
    },
    "relevant_docs": {
        "07360df9-c77f-4f28-808b-2852449492e2": [
            "927cd889-78dd-4741-b234-8275a79e3779"
        ],
        "6a5c7c08-3000-4a7a-a629-d430c52c667f": [
            "d13c946f-fa35-474c-8012-216557272c04"
        ],
        "3d25297e-1d02-4d70-ab88-acb8b97fc132": [
            "1de2b2c9-8f97-4ecd-8e44-64effb460e4e"
        ],
        "974dd1c6-85d9-4dbc-9f5d-d940b970a48a": [
            "f71cbf17-64bb-49ea-9eb8-c51201e7f6ff"
        ],
        "9dcec08f-b484-44c3-a660-abb0853476ae": [
            "a158b488-05d0-448d-9029-f94cda0ee615"
        ],
        "037ce451-c7f2-406a-bff8-b2a5f70c6fc4": [
            "2d299975-4df5-4d5e-8f49-8bf0ff58cbc5"
        ],
        "dafd92ee-bf73-474f-be16-dc26bdf58bf1": [
            "f5fce271-577f-4ee0-b680-a8d434aabbc9"
        ],
        "69d9c753-a89c-4755-871d-873ed358291b": [
            "92d469ee-6e97-4eb8-b37b-bc153a4f4aef"
        ],
        "2ccbd314-7806-4f64-89f9-27632d2ae583": [
            "7e9bbfc5-d7ac-489e-8d36-5685749fcb10"
        ],
        "82580531-4b57-41c9-b433-2fa2d45091ea": [
            "15071151-8a29-4c6a-a652-a5e117c8e1f0"
        ]
    },
    "mode": "text"
}