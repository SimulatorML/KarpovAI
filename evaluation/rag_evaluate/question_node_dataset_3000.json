{
    "queries": {
        "ad7fd8fe-f70b-4fd2-ba1c-c8ef4765272c": "\"Какие основные компетенции у Data Analytics и ML-инженера?\"",
        "a1630906-79d8-40dd-a2fc-36804caf170b": "\"Какие модули программы обучения аналитиков данных наиболее важны?\"",
        "6481f74c-887f-47ee-b46c-f833e86200b2": "Как аналитика данных связана с машинным обучением и продуктовыми метриками?",
        "1feb73cc-3c39-4746-9085-eec0b010284e": "\"Какие блоки курса инженерии данных рассматриваются в тексте?\"",
        "12662d67-26b0-44dd-980f-8a5e31ce99d7": "\"Какие инструменты используются для Data Security и Privacy?\"",
        "0e7673dc-2c78-43b4-a5a4-4b1937830aa2": "\"Какие форматы данных может считывать библиотека Pandas?\"",
        "fec89b93-6f98-4689-aea9-8a99c38e109d": "\"Какие основные инструменты аналитика данных обсуждаются на вебинаре?\"",
        "bd103bde-bb62-475e-8444-1c79dfaf44e0": "\"Какие функции выполняет инструмент Redash при работе с базами данных?\"",
        "7053df67-e626-46ee-b70c-fd47b3f9fba8": "\"Какие операции можно проводить с числами в Python?\"",
        "6ce6aae0-299d-42ee-9bc2-70b85e767e20": "Какие библиотеки Python используются для анализа данных?"
    },
    "corpus": {
        "489beae3-401b-4c19-bef8-8e09c99f3390": "Привет, меня зовут Саша и сегодня мы с вами поговорим о разных профессиях в Data Science. Data Science это область знаний на стыке статистики, бизнес-аналитики и программирования, в основе которой лежит работа над данными. Первичная обработка данных, построение гипотез, их проверка, построение АБТ-тестов, обучение ML-моделей, построение хранилищ данных, все это включает в себя Data Science. В небольшом стартапе всем этим может заниматься один специалист универсал, однако в более крупных компаниях данными занимается целый отдел, который может включать в себя ML-инженеров, Data-инженеров и Data-аналитиков. Сегодня мы как раз разберем, кем являются все эти специалисты, чем они отличаются друг от друга и какая область Data Science подходит именно вам. Начнем разговор сегодня мы с Data Engineer, поскольку его работа одна из самых важных. Data Engineer отвечает за качество и доступность данных, также занимается разработкой DVH-хранилищ и построением пайплайнов обработки данных. Работа Data Engineer очень важна, поскольку любая работа с данными подчиняется принципу garbage in, garbage out, то есть даже любая самая продвинутая аналитическая система будет совершенно бесполезна бизнесу, если на вход будут подаваться мусорные данные. Основной компетенцией Data Engineer это как раз проектирование и поддержка традиционных либо облачных DVH и автоматизация телепайплайнов. Для начала Data Engineer должен уметь хорошо кодить на питоне, понимать SQL, а также разбираться между различиями в построении SQL и NaoSQL баз данных. Более продвинутый Data Engineer уже работает с большими данными, а значит использует Kafka, Hadoop и Spark, однако ему совершенно не обязательно разбираться в тонкостях машинного обучения или проверке статических гипотез. Data Engineer не так сильно погружен в доменную область, как остальные его коллеги, он скорее строит техническую инфраструктуру под нужный других членов команды. Дальше мы поговорим о Data Analytics. Этот специалист как раз очень сильно взаимодействует с бизнесом, потому что его задача как раз отвечать на вопросы клиентов. Как новая рекламная кампания влияет на продажи или почему от нас уходят наши клиенты? Аналитик как раз отвечает на эти вопросы исходя из данных, а не полагаясь на свое какое-то жизненное понимание или интуицию. Основные компетенции Data Analytics это построение статических гипотез, их проверка, проведение АБ-тестов, визуализация, построение дэшбордов и наглядная визуализация результатов для бизнеса. Такой специалист должен иметь достаточно сильную математическую базу, в отличие, например, от Data Engineers, поскольку ему приходится работать со статическими гипотезами и проверять их. Технический стек такого специалиста достаточно обширен и может варьировать в зависимости от доменной области, сложности задач или опыта самого специалиста. Сегодняшний стек такого специалиста составляет Python, SQL и биосистемы наподобие Tableau. Реже могут использоваться простые ML-алгоритмы типа линейных или логистических регрессий, также язык R и совсем крайне в случаях Excel. Дальше мы поговорим про ML-инженера. Задачи бизнеса могут быть достаточно сложными, когда умений Data Analytics уже не хватает, тогда приходится строить более сложные системы. Этим как раз занимается ML-инженер. Роль ML-инженера в команде — это как раз решать специфические технические задачи, которые ставят ему его коллеги-аналитики. Если просто аналитик пользуется какими-то простыми ML-алгоритмами типа регрессий, то ML-инженер строит уже более сложные решения на основе деревьев решений, кластеризации или композиции этих алгоритмов. Сфера его ответственности — это как раз построение моделей, их обучение, развертывание и поддержка в продакшене. По сути, ML-инженер — это Python-разработчик, который обладает специфическими знаниями в области данных. Например, он использует классический DSTack — это Python, NumPy, SocketLearn или SciPy. В отличие от Data Analytics, ML-инженер не строит статистические гипотезы, он их уже проверяет. При этом в проблемы бизнеса он может быть включен достаточно поверхностно. И теперь пришло время поговорить о Data Scientist. Казалось бы, любого специалиста в области Data Science можно назвать Data Scientist, но как-то так сложилось, что Data Scientist принято называть специалистом, который работает на стыке Data Analytics и ML-инженерии. Data Scientist — это человек, который хорошо пронимает проблемы бизнеса, но при этом уже умеет строить более сложные ML-решения и на основе этого помогает бизнесу отвечать на его вопросы. Можно сказать, что Data Scientist — это аналитик с базовым знанием ML-инжиниринга. Так же, как Data Analytics, он строит гипотезы, их проверяет и помогает интерпретировать результаты для бизнеса, чтобы помочь получить ответы на какие-то вопросы. При этом простые модели Data Scientist может обучать сам, но иногда ему также нужна помощь ML-инженера. Напоследок хочется сказать, что мое, либо какое-либо другое разделение специальностей в области Data Science достаточно условно. Оно может различаться в зависимости от компании к компании, от страны к стране, а в маленьком стартапе отсутствовать вовсе. Кроме того, нельзя забывать про множество подвидов Data специальностей. Например, это могут быть BI-аналитики, продуктовые аналитики, разработчики алгоритмов нейронных сетей. Отсюда следует, что при поиске работы внимательно читайте описание вакансии, не бойтесь задавать уточняющие вопросы, попросите рассказать вам о стеке и конкретно о задачах, которые вам нужно будет решать. Также помните, что никогда не поздно сменить траекторию развития, так как знания, полученные в одной области Data Science, могут быть вам полезны и в другой.",
        "db5f2d5d-f989-4504-940a-54aca7ca37eb": "То есть очень важна комбинация и хардскиллов, и каких-то пониманий о бизнес-процессах, и софтскиллов, но об этом чуть дальше. Поэтому вот Airflow — это такой заключительный, так скажем, финальный модуль, посвященный программированию, который научит вас выстраивать уже некоторые такие, как это принято звать, пайплайны по работе с данными и оркестрировать уже довольно сложные задачи по предобработке и каких-то хитрых манипуляций с данными при помощи питона. Закончив с программированием питона, мы переходим к блоку, который называется SQL. SQL — это на самом деле тоже джентльменский набор, без этого никуда. Как я сказал, что если в 90% случаев в резюме от вас будут ожидать, что вы знаете питон, то вот в 100% случаев от вас будут ожидать, что вы знаете SQL. База данных. База данных — это основной способ хранить данные в индустрии. И на самом деле есть огромное количество различных баз данных. Есть там, вы слышали, наверное, MySQL, Postgres, Clickhouse, Vertica. Список можно продолжать довольно долго, но это, пожалуй, самые популярные. И все они объединены тем, что есть некоторый такой общий, так скажем, подход, который позволяет нам взаимодействовать с информацией, которая хранится в этих базах данных. Извлекать эту информацию, как-то трансформировать, этот подход называется SQL. Это некоторый такой целый набор команд, инструкций, при помощи которых вы научитесь извлекать данные из баз данных, работать с ними. И, в принципе, очень часто это выглядит как раз-таки такой совместной командной работой. Вы при помощи SQL забираете данные из баз данных и после этого, например, анализируете их на питоне. То есть помимо того, что все модули, которые в этой программе включены, они довольно важны для того, чтобы стать хорошим аналитиком, мы еще и постарались очень явно показать некую преемственность этих модулей, что они не разрозненные какие-то между собой, а они все в целом создают единый такой набор инструментов, которые крайне необходимы аналитику. И SQL, пожалуй, я даже не знаю, тяжело сказать, кто сильнее, питон или SQL, но, в общем, и питон, и SQL – это такой basic hard skills, которые просто необходимо иметь любому человеку, который хочет заниматься анализом данных на сегодняшний день. Разобравшись с такими тяжелыми hard skills, как питон и SQL, мы переходим к не менее интересной теме, которая называется введение в теорию вероятностей. И это как раз-таки показывает тот факт, что когда вы хотите стать аналитиком данных, вы должны понимать, что аналитик данных – это действительно, ну, в своем роде такой человек-оркестр, то есть вы должны уметь и программировать, и работать с базами данных, и понимать бизнес, и понимать какие-то математические основы. И как раз-таки теория вероятностей призвана показать вам, как в целом мы, как аналитики, можем работать с окружающими нас величинами, как с некоторыми математическими конструкциями. Как можно проверять гипотезы, как можно отличать случайные события от закономерных, и для того, чтобы у вас сформировалось более глубокое понимание прикладного инструмента статистики, нужно понимать некоторые базовые вещи, которые называются теорией вероятностей. Этот модуль будет заведомо кратким, но довольно насыщенным. Этот модуль также читаю я, и как раз-таки здесь хочется показать некоторые такие очень основополагающие вещи, которые в целом позволят вам смотреть как бы на мир немного с точки зрения математика, немного с точки зрения человека, который может описывать различные события, которые могут, не знаю, на первый взгляд показаться какими-то случайными, несвязанными, как последовательную историю каких-то распределений, случайных величин, гипотез. Это довольно важный навык для аналитика, потому что даже очень часто можно встретить такую фразу, как аналитическое мышление. И если вот это не очень понятное понятие, что значит аналитическое мышление, то во многом оно как раз-таки основано на том, что аналитик довольно хорошо разбирается в теории вероятностей и в математической статистике. А разбираться в математической статистике без теории вероятности очень тяжело. Поэтому это такой первый модуль, который является введением в следующий модуль Теория Вероятностей. Ну и, соответственно, после того, как мы разберемся с основами Теории Вероятности, мы переходим к модулю, который называется «Статистика» и «Обытестирование». Здесь, я думаю, тоже очень важно сделать акцент на том, что аналитик и обытесты — это в каком-то смысле практически синонимы. Обытестирование — это один из самых надежных и общепринятых способов проверять различные бизнес-гипотезы. И для того, чтобы разобраться, как работают обытесты, маленький пример, вы выкатываете новый дизайн в вашем приложении и хотите понять, правда ли, что новый дизайн работает лучше, чем старый. Что он сейчас работает, например, увеличивает среднее время сессии, которое пользователь проводит в приложении. Для того, чтобы проверить такую, казалось бы, на первый взгляд, простую гипотезу, нужно понимать, как выстраивать обытесты, как правильно проводить обытесты, какая математика лежит в основе того, что мы можем сравнить два варианта дизайна и с уверенностью утверждать, что один вариант дизайна лучше, или на самом деле мы можем утверждать напротив, что нет никаких статистически значимых различий и нужно искать какое-то другое направление для развития бизнес-идей, которая была. Поэтому статистика и обытесты — крайне важный модуль. Здесь даже не знаю, что добавить. Как я сказал, в каком-то смысле это прямо синонимы аналитик и статистика и обытесты. Этот модуль выстроен следующим образом. Мы сначала с вами поговорим про то, как устроена статистика, именно математическая статистика, как подраздел математики. Познакомимся с различными распределениями, теоремами, статистическими критериями, и потом сразу же заземлим эту теоретическую информацию на прикладных примерах того, как обытесты проводятся в индустрии. После того, как мы познакомимся с математической статистикой и обытестированием, начинается блок визуализации. Как вы могли догадаться, тоже очень важный модуль. Ну, я с этого начал, что у нас нет модулей, которые можно, в принципе, было бы выкинуть и не проходить. Все они как бы по кусочку, как мозаичка, собирают из вас полноценного такого специалиста, всесторонне развитого. Визуализация – это самый заключительный этап аналитики. Когда вы пришли в компанию, когда вам дали какую-то бизнес-задачу, и когда вы, как аналитик, используя Python, SQL, MatStat, Airflow, Edit, решили ее, в 90% случаев именно финальный дашборд с графиками, диаграммами, которые показывают результаты, является финалом работы аналитика, и научиться грамотно визуализировать данные – это не просто эстетический вопрос, это вопрос, который позволяет вам максимально прозрачно, быстро и успешно доносить результаты, которые вы получили в рамках своего анализа, до огромного количества пользователей внутри компании.",
        "740ab60f-f15f-432b-b8a3-8652b73e8508": "Всем привет, это снова Анатолий Карпов, мы на третьем занятии, и сегодня нет времени объяснять, начинаем заниматься аналитикой. Мы с вами разобрали два базовых таких элемента, поговорили про основы синтаксиса Питона, про какие-то ифы, элсы, форы, листы, списки, словари. И на этом сейчас не будем останавливаться. Я уже пояснял, почему. Мы сейчас пойдем дальше, сразу в анализ данных, и как только нам будут встречаться задачи, которые требуют какого-то более глубокого погружения в структуру самого синтаксиса языка, будем на этом подробно останавливаться и разбирать. Но именно в связке с какими-то сразу прикладными задачами. Например, скоро у нас будет занятие, которое будет посвящено считыванию очень грязных данных, зашумленных, когда готовые библиотеки нам помочь уже не могут. И вот там как раз придется попрогать на Питоне. Вот заодно и научимся. А сейчас мы продолжаем наше стремительное погружение в аналитику. Как я говорил, наш курс отличается интенсивностью, поэтому сегодня сразу же будем решать уже аналитическую задачу. Поговорим про такую часть аналитики, как анализ пользователей, их поведение. Поговорим про лояльность пользователей. И сразу же выясним, как можно такие задачи решать при помощи Пайтона, при помощи Пандаса. И, в принципе, обсудим тоже довольно много интересных аналитических вопросов. Итак, мы снова в Юпитер-хабе. Обратите внимание, я еще раз создал папочку Lesson 3. Сюда положил данные Lesson 3 Data. Но, опять же, эти данные доступны для вас в папочке Shared. Здесь будут появляться все материалы, которые мы используем. Поэтому можно, в принципе, пользоваться этой папкой, если вам нужно считать какие-то данные, посмотреть какие-то конспекты. Там все будет. Я создал еще один Юпитер-ноутбук 3. И, как обычно, мы начинаем с библиотек. В данном случае мы пишем ключевое слово import pandas spd. Выполняем эту ячейку. Мы видим, что все выполнилось без ошибок. Значит, эта библиотека и ее функции доступны. И, в принципе, вот эти вот импорт библиотек — это такое первое слово в анализе данных. Всегда любой аналитик начинает свою работу с импорта нужных библиотек. Как я говорил, работать с сырым питоном уже так давно никто не делает. И, для начала, опять же, считаем данные. Мы уже знаем, что у нас есть функция pd read.csv. Вы можете начать писать путь к файлу. Для этого поставим кавычки. Выполним вот такую команду. Слэш. Вот такая вот подчеркнутая линия, волнистая. Слэш. Это означает, что мы находимся как бы в самом начале нашего пути, где лежат файлы. Выберем здесь папочку shared. Видите, сразу подставился, на самом деле, как бы по-настоящему полный путь. И здесь выберем lesson tree data. Это будут те данные, с которыми мы работаем сегодня. Опять же, обратите внимание, что у нас будет ошибка с кодировкой. Но ничего страшного, давайте к этому привыкать. Далеко не всегда у нас кодировка в нужном формате, поэтому нужно уметь считывать данные, которые приходят к нам в любых форматах. Собственно говоря, чаще всего, как я сказал, это будет либо utf-8, либо windows-овская encoding, и никаких проблем с этим не возникает. Итак, для начала посмотрим на данные. И сейчас я расскажу, что мы будем делать. Смотрите, сегодня мы с вами решаем задачу, которая связана уже с аналитикой пользователей. Мы посмотрим данные, которые приходят из e-commerce, ритейла. Это данные по покупкам. Это, пожалуй, такая довольно большая и огромная часть data science, аналитики. В электронной коммерции, в покупках, в магазинах, в заказах аналитика, пожалуй, стоит ключевой точкой. Практически все сейчас крупные игроки используют data science, аналитику, машинное обучение в очень большом спектре применений. И, соответственно, если говорить больше про машинное обучение, то здесь возникают задачи, которые связаны с персональными рекомендациями. Когда, например, вы покупаете какие-то покупки на озоне или на aliexpress, наверное, вы заметили, что внизу вам всегда еще предлагают какие-то рекомендованные товары. Возможно, далеко не всегда это выглядит как идеальная рекомендация, но data scientist стараются и пытаются, основываясь на ваших предпочтениях, подобрать такие товары, которые вы купите максимально чаще. И вот это сразу переводит нас к очень важному вопросу, как аналитики связаны с data scientist, с такими ребятами, которые занимаются прямо хардкорным ML. Ребята, которые занимаются хардкорным ML, которые пишут нейросетки, которые пишут всякие сложные алгоритмы, они далеко не всегда в команде очень хорошо разбираются именно в продуктовых деталях. Что наши за пользователи, что у нас за продукт, какой у нас показатель основных продуктовых метрик, сколько пользователей обычно пользуются нашим продуктом, сколько они совершают покупок, что это за пользователи, как долго они проводят время в нашем сервисе. И у них скорее есть такая уже более локальная задача. Мы им говорим, вот смотрите, есть там ребята, которые что-то покупают, предскажите им при помощи нейросети и машинного обучения, что они купят еще. А аналитики в этой истории, они в каком-то смысле подготавливают почву. Мы можем, мы аналитики, исследовать продукт, выделить какие-то паттерны поведения, понять, что есть пользователи, которые довольно часто проявляют определенные паттерны поведения в взаимодействии с продуктом и основываясь на этом сформировать ряд гипотез, которые позволят продукту увеличить свои метрики. Например, увеличить доход каждого пользователя. Эта метрика называется по-разному, но самый простой способ это ARPU – Average Revenue Per User, то есть сколько мы зарабатываем на пользователей. И вот, когда аналитики поисследовали, нашли определенные точки роста, вот очень часто на помощь подключаются дата-сайентисты. Мы понимаем, что есть люди, которые, например, очень часто совершают одну покупку и через какое-то время за ней совершают еще одну покупку. И вот мы такие говорим, ага, а можно предсказать, что это за покупка? А можем мы сразу ему показать эту рекомендацию в момент совершения первой покупки? И вот здесь зачастую аналитик не обладает всем спектром именно такого же хардкорного машинного обучения. Он приходит к дата-сайентистам и говорит, смотрите, у меня есть задача, у меня есть пользователи, которые купили сначала одну покупку, потом они собираются купить вторую, вот мы уже знаем, что они это делают. А мы можем заранее это предсказать? И дата-сайентисты берут себе эти данные и начинают там устраивать всякую магию с нейросетками, дип-лернингами и всякими сложными математическими штуками. Но обратите внимание, что как аналитик без машинлернера не смог бы запилить такую модельку очень классную, так и машинлернер без аналитика в целом, возможно, бы и не увидел, что именно здесь нужно сложное машинное обучение применить.",
        "fd55a8da-f7b9-46e7-ad8f-26e9b243948e": "Добрый день, меня зовут Ермаков Евгений, я являюсь руководителем хранилища данных в Яндекс.Го, это такси, драйв, еда и лавка, и являюсь хедлайнером курса инженерии данных в Karpov Courses. И в рамках этого небольшого видео я расскажу, о чем мы хотим поведать в рамках нашего курса, на чем хотим расставить акценты, и что вы узнаете, пройдя весь наш курс и все наши блоки. Начну я издалека с хранилища данных. Что такое хранилище данных? Ответ на этот вопрос на самом деле есть, и вы узнаете его в рамках нашего курса, но если в среднем спросить даже образованного IT-специалиста, он обрисует что-то представленное на слайде. У нас есть система источники, все это каким-то образом попадает в непосредственно хранилище данных, такой большой черный ящик, из которого уже умные аналитики своими запросами достают или операционную отчетность, или визуализируют данные, или делают что-то более сложное, вроде перспективной аналитики. На самом деле это, конечно, не так, все немножечко сложнее. Вот, например, то, как представляет архитектуру хранилища данных, Damodembook, это такой достаточно известный в узких кругах хранилище данных стандарт по управлению данными в предприятии. Если посмотреть на всю эту картинку сверху, то здесь есть и аналитическое хранилище, которое в центре, и которое разделено на всевозможные слои и предоставляет информацию для аналитиков. Здесь есть и озеро данных, или BigData, которое находится как бы отдельно от аналитического хранилища, но его результаты в том числе используются на входе этого аналитического хранилища. Здесь есть и всевозможные ETL и ELT процессы, которые каким-то образом оркестрируются, и благодаря которым данные поступают от источников непосредственно к аналитикам. Здесь есть и такой аспект, как проектирование хранилища данных. Что находится в этом центральном слое, что за витрины данных в хранилищах, что за кубы. И, безусловно, оно у меня последнее в очереди, но, наверное, первое по важности это для чего все эти данные, кому они нужны, как они будут анализироваться, как визуализируются и кто с ними работает, для чего используют и какие решения принимают на основе данных. Исходя из этих блоков, мы разбили наш курс на следующие этапы. Первый — это проектирование ДВХ и системный анализ. Здесь может быть логичный вопрос от слушателя, зачем инженеру данных понимать ДВХ в целом, архитектуру, какие-то аспекты системного анализа, он же этим не занимается. Вопрос справедливый, но отчасти. Если вы будете, например, в стартапе, то все обязанности, в том числе и системный анализ, и архитектура хранилища данных будут полностью на вас, как на инженере данных. Если вы работаете в небольшой команде, например, из 4-5 человек, у нас в Яндексе есть ряд бизнес-юнитов, в которых небольшие команды, то аспект системного анализа будет тоже на вас, как на инженере данных. Если вы работаете в крупной организации, например, в нашем хранилище данных, соотношение межсистемного аналитика с инженером данных порядка 1 к 3-4. Вам, как непосредственному исполнителю задач, человек, который это делает руками, нужно понимать, а что вы делаете, как, зачем, почему, почему это спроектировано именно так, находить узкие места и возвращать неправильную задачу системному аналитику. Следующий блок — это реляционный СОБД, в том числе массивно-параллельный Грен-План. Здесь мы рассмотрим архитектуру массивно-параллельных СОБД, как работают распределенные запросы и сложную оптимизацию этих запросов. Следующий блок — озеро данных. Бигдата, Hadoop, отказы устойчивого всего кластера, непотоковая обработка поверх этого. И следующий, четвертый блок — ETL-платформа. Здесь мы посмотрим на ETL-платформу Airflow, посмотрим на код-дриван ETL, типичные ETL ELT задачи и DAG на базе Airflow. И вот эти три блока — это базис инженера данных. Вы должны понимать, как работают реляционные базы данных, в том числе массивно-параллельные, потому что при большом объеме данных аналитические хранилища строятся на них. Вы должны понимать, что такое озеро данных, как с ним работать, и как не превратить его в болото данных, и как всем этим оркестрировать, как прогружать гигантские объемы данных между всеми этими источниками, как делать так, чтобы эти данные доезжали в нужном объеме в нужное время и с нужным качеством к аналитикам. Все это может находиться как в in-house решениях на железе, так и в облаке. Поэтому следующим блоком у нас будет облачное хранилище данных и инструменты для построения DWH в облаке. Это важная тема. Все базисы и все принципы построения DWH там те же, но меняются некоторые важные нюансы. И последний блок, он опять у меня последний, но тем не менее, он не последний по важности. Это биоинструменты. Вам как инженеру данных важно понимать, зачем вы делаете ту или иную витрину, что с ней будет происходить, как аналитик это будет визуализировать и что с ним делать. Без этого понимания можно делать задачи механически, не предоставляя нужного на самом деле результата. Делая то, что хочет заказчик, но не то, что ему действительно нужно. Что мы требуем от вас как отслушателей на старте? Мы ожидаем, что вы хорошо знаете SQL или SQL, хорошо знаете Python. Что такое хорошее знание одного и другого? Для SQL это базовый синтаксис, это серебро, руба и хевинг, это должно быть как коча наш. Понимание всех видов джойнов под запрос, умение их комбинировать и важен последний пункт, оконные функции. Я по своему опыту скажу, что много кто их знает, но мало кто их умеет применять. Это очень тонкая и важная разница, которую вы должны у себя прощупать, задать себе вопрос, действительно ли вы умеете эти оконные функции применять легко и просто. Для Python нам нужен базовый синтаксис, понимание базовых структур, список, словарь, картеж, все это не должно вызывать вопросов. И основу объектно-ориентированного программирования. Наследование, классы, объекты, все это должно быть легко и просто. Пример задачи, чтобы оценить насколько вы подходите под наши требования. Суровые требования, но справедливые. По SQL пример задачи, по данным об оценках студентов, всего три поля, курс, студент, оценка, вывести 5 студентов по каждому курсу. Если у вас прямо сейчас не родился ответ на этот запрос в виде SQL, то наверное стоит взять и повторить, в том числе и оконные функции. А по Python это какие-нибудь простые относительно алгоритмические задачки, например, проверить, что входная строка это palindrome, или написать join двух массивов за сложность меньше, чем от m на n. Ну и последний слайд в этом небольшом видео. Это наши преподаватели, вы с ними познакомитесь, они работают, наверное, без лишней скромности в лучших компаниях на российском рынке, в лучших IT-компаниях. Я уверен, вам с ними понравится. На этом все, спасибо за внимание и подключайтесь к нашему курсу.",
        "46608081-7a4e-493a-b5b9-9205d1645cbf": "Во-первых, желательно понимать, что это вообще такое, что это за термин, чем он отличается от Data Management, например. У нас в курсе, конечно же, это есть. Здесь у нас есть разные подходы, которые позволяют нам организовать данные процессы. Это такое понимание, как Data Catalog, термин, скажем так, паттерн некоторый. И Data Lineage. Это разные подходы, которые позволяют вам следить за вашими данными. То есть понимать, что у вас есть в системе, какую структуру они имеют, например. И как эти данные, если мы в частности говорим про Data Lineage, то есть как эти данные попали к вам в ту или иную, например, витрину. Из какого источника и через что они прошли. И инструменты, которые позволяют нам реализовывать данные подходы, это DBT, Atlas, Data Hub и Amundsen. То есть DBT, как я уже говорил, это некоторый многофункциональный инструмент, который позволяет нам не только, собственно, обрабатывать данные, но и ввести по ним некоторую, скажем так, описательную документацию того, что у вас присутствует в вашем хранилище. Это, собственно, касаемо раздела Data Governance. Давайте теперь перейдем к следующему разделу. Это Data Security и Privacy. Здесь у нас тоже есть разные направления. Первый у нас с вами — это Legal Compliance. Сюда входят PCI, API, PHI. То есть вы должны понимать, в чем различие между этими терминами. Это термины о типах данных, которые у вас присутствуют, о пользователе. Будь то это сведения о финансах этого пользователя, может быть о его транзакциях, оплатах, которые он проходил в счетах. API у нас это сведения о его персональные данные, которые вы тоже должны определенным образом защищать и хранить у себя. И PHI — это так называемая, скажем так, врачебная тайна. То есть есть системы, которые работают и существуют непосредственно для врачей. И они также работают с определенной информацией, которую нужно защищать определенным образом и уметь с ней работать. GDPR — это некоторый стандарт, который говорит нам о том, как нам нужно хранить данные о пользователе и предоставлять некоторые дополнительные функционалы для того, чтобы, например, эти данные о пользователе удалить. Если, собственно, пользователь изъявил свое желание. И разные прочие, скажем так, правовые нормы, которые тоже иногда нужно соблюдать в вашей системе, хотите вы этого или нет. Key Management — это управление разными ключами, которые у вас присутствуют в системе, ключами доступа. Access Management — это управление непосредственно самим доступом. То есть, в принципе, некоторая такая ролевая модель. То есть какие у вас присутствуют роли в вашей компании или в вашей системе, как вы можете получать доступ к тем или иным данным и как это у вас, собственно, непосредственно будет организовано. Data Masking — это маскирование данных, то есть обезличивание данных. Также, если вы поставляете данные каким-то внешним источником, то вам, возможно, иногда будет приходиться маскировать данные. Вы должны понимать тоже, как это устроено и как это работает, какие подходы применяются в этом направлении. Токенизация данных у нас далее — это Data Tokenization. Тоже некоторый подход, который позволяет вам защитить ваши данные для того, чтобы, например, обработать где-то эти данные на незащищенной инфраструктуре или, может быть, предоставить опять же эти данные определенным образом какому-то стороннему клиенту для того, чтобы он вам их потом вернул и вы могли бы их восстановить. И непосредственно шифрование, то есть Encryption. Здесь, собственно, есть некоторые алгоритмы, которые достаточно широко применяются, и вам тоже стоит уделить им внимание. Это RSAIS, то есть это так называемое двустороннее и одностороннее шифрование. И OpenPGP, как некоторые тоже относительно такой стандарт в шифровании, подход, который применяется. Это касаемо непосредственно безопасности наших данных. В разделе Data Quality наиболее популярные формворки, которые сейчас до сих пор используются — это Great Expectations.\noperations, которые позволяют нам строить разные правила для проверки наших данных, их, скажем так, оценки, вот, и PIDQ, который тоже используется. Apache Griffin является уже достаточно таким legacy решением, я его уже давно не видел на проектах, вот, и он достаточно сложный в настройке. Проще использовать те походы, которые действительно сейчас популярны, они являются более легковесными, и в них проще разобраться. Далее у нас это визуализация, да, здесь, собственно, непосредственно о BI, это структуры данных, которые специфичны для BI, да, то есть вам точно нужно знать, понимать, как вы можете с ними работать, да, то есть здесь можно сказать, конечно, про что угодно, но здесь важно понимать те структуры, которые у нас присутствуют и которые нам упрощают визуализацию данных, вот, то есть не всегда то, что вы храните, действительно удобно визуализировать. Непосредственно сами подходы в даты визуализации, то есть какие у нас присутствуют, скажем так, компоненты для визуализации, разные чарты, так сказать, диаграммки, да, которые нам позволяют определенным образом визуализировать нам информацию, да, одну информацию удобнее читать там одной диаграммой типа там пирога, а другую, например, если это какие-то time series данные, да, нам удобнее читать с отображением временной шкалы и непосредственно с некоторым сдвигом для того, чтобы мы могли оценивать, например, изменения, вот, ну и здесь BI-tools, которые нам необходимы, собственно, для этого это Tableau, Qlik, Looker, тоже достаточно популярный инструмент, и Superset, Superset является open source решением, одним из таких, с которым достаточно легко разобраться, все остальные являются более сложными решениями в плане BI, вот, в том плане, что, скажем так, они предоставляют вам больше возможностей, больше функционала, но и вам придется тоже достаточно больше потратить времени для того, чтобы с ними разобраться и построить действительно отличный дашборд, но эти решения являются сплатными. Так, на этом все у нас касаемо BI и визуализации данных, и последний раздел, который также, собственно, полезен дату инженеру, это машинное обучение, здесь у нас сюда входит некоторая терминология, да, что такое обучение с учителем, обучение без учителя, как происходит у нас оценка метрик, что такое, какие вообще существуют метрики для разных задач, класса задач, вот, классические алгоритмы машинного обучения, линейная регрессия, DC-режим 3, Random Forest, K-Means, K-NN, хотя бы иметь просто некоторое представление о том, что это такое, как это работает, это даст вам представление о том, какие базовые модели могут быть реализованы в ваших задачах и, собственно, непосредственно даст некоторое такое базовое понимание в том, что вообще делают другие специалисты, да, такие как машинный инженеры, дата-аналитики, дата-сантисты, которые занимаются непосредственно моделями. Также у нас здесь раздел «Практик», да, который непосредственно свойственен машинному обучению, это понимание, что такое feature store, да, его цели, как некоторый такой паттерн по работе с, скажем так, процессами построения новых данных. Model tracking — некоторые специфичные инструменты, которые позволяют нам трекать модели, да, и вести некоторый реестр, собственно, моделей. Model serving — для смещения моделей на окружении и подготовки их к выведению в продакшен, да, то есть, ну, фактически это есть выведение в продакшен некоторые.",
        "1554b1d9-a57b-4508-9536-d388def0ebe1": "План у нас следующий Сейчас поработаем с данными, покопаемся в Pandas После этого поотвечаем на ваши вопросы Не обязательно по Pandas, в принципе, мы тут периодически собираемся Если кто-то пришел к нам первый раз, напомню, что мы стараемся раз в неделю, раз в две недели Всегда проводить вебинары, где у вас будет возможность любые вопросы мне задать Задать любые вопросы нашим коллегам, которые со мной тоже занимаются аналитикой, преподаванием аналитики И, соответственно, всегда можно будет какие-то вопросы проговорить Соответственно, сначала разбираем Pandas, потом отвечаем на вопросы Ну а в конце я опять напомню вам, расскажу, что за проект мы делаем, как мы обучаем анализу данных И как нам можно присоединиться, если вы хотите тоже поучаствовать в наших образовательных проектах Ну что ж, а теперь давайте начинать Итак, я вижу, что, в принципе, вы прекрасно видите мой экран Мы с вами будем говорить на языке Python, на языке Pandas И вот это вот Юпитерхаб, соответственно, Юпитерноутбук Один из самых популярных и простых интерфейсов для работы с кодом, с данными в Питоне Такая вот интерактивная среда, где можно запускать какие-то запросы И вот обратите внимание, первой же ячейкой мы делаем команду import pandas spd Соответственно, мы говорим, что мы будем с вами использовать библиотеку Pandas И сокращённое название будем в неё использовать PD Дальше идёт несколько импортов библиотек для визуализации Seaborn и Matplotlib — это самые популярные, самые частые данные для визуализации в Питоне Соответственно, с ними сегодня тоже, если успеем, поработаем И, соответственно, сами данные, которые мы читаем, это данные про интернет-магазин Про покупки и продажи в интернет-магазине Обратите внимание, я вызвал команду pandas read csv Указал ссылку прямо на архив с данными Соответственно, опять же, там в чате пишут Я в конце все ссылочки, все данные скину, чтобы у вас была возможность И самостоятельно уже за мной, повторяя, эти данные покопать И указал кодировку, в которой эти данные соединены Что вообще такое pandas? Все очень часто шутят, что pandas — это от слова «панды» Я думаю, разработчики, возможно, имели это в виду Но, по крайней мере, от этого открещиваются и говорят, что pandas — это panel data И действительно, pandas позволяет нам работать с данными Вот в таком классическом варианте, когда у нас есть некоторые табличные представления Мы это называем датафреймами в аналитике данных У нас есть какие-то колонки Каждая колонка — это определенная переменная, определенная величина Определенная фича, если мы говорим про data science Соответственно, мы можем здесь увидеть, например, в первой колоночке у нас будет номер заказа Во втором — код заказа, описание этого товара, количество, дата покупки, цена за один товар Собственно говоря, если у нас будет тут 6, надо просто будет умножить ID кастомера и country Соответственно, pandas — не что иное, как просто очень удобная и такая простая обертка Которая позволяет нам представлять данные в питоне, которые мы обычно храним в CSV, в Excel, в каких-то таблицах Вот тоже в такой табличной форме Единственное, сразу же можно сделать... Да, и поэтому как бы самое простое, чтобы понять, что такое pandas Pandas — это просто способ работать с табличными данными Вот если мы сочетали pd, read, csv и какой-то data frame И просто вывели на печать вот эту переменную, в которой мы сохранили данные retail Обратите внимание, что у нас просто вот мы видим наши данные Вот они в колонках, вот в строчках Для тех, кто хочет чуть-чуть более углубиться, что же такое pandas на самом деле Позволю себе сделать маленькое лирическое отступление Обратите внимание, что мы в pandas можем делать вот такие вот запросики Например, открыть квадратные схобочки и соответственно указать, например, название колонки Обратите внимание, что так выведен только нужную нам колонку Или, например, вывести колонку с country Причем колонки — это непростые Мы не просто можем, например, вывести нужную нам величину Но и сразу что-то с ней сделать Допустим, рассуммировать И в этом смысле надо понимать, что pandas объединяет в себе, так скажем, два основных питоновских принципа Во-первых, можно сказать, что pandas — это некоторый словарик Где просто под каждым ключом, а ключ — это название колонки, хранятся нужные нам данные Например, под ключиком quantity хранится колоночка quantity Под ключиком customerid хранится колоночка customerid То есть pandas — это такой словарик, питоновский словарь, дикт, в котором хранятся нужные нам данные И сами данные сохранены тоже не самым простым способом То есть в питоне есть стандартные листы Мы, например, можем создать список 1, 2, 3 — целые числа Или список с, например, названием стран Прошу прощения Вот И более того, мы даже могли бы объединить эти списки в один список Учитывая, что это такой многомерный список Но, понятное дело, что работать с такими данными неудобно И вот просто pandas, он как бы объединяет вот такие вот разные массивчики В которых хранится нужная нам информация Где каждый массивчик — это колонка, как в Excel-е столбик И сами по себе эти колонки тоже не простые Они, на самом деле, являются таким типом, который называется pandas series\nPandaSeries — это просто такой, в каком-то смысле, продвинутый список. Вот видите, обратите внимание, теперь, когда я сделал просто обычный список и перевел его в PandaSeries, получился такой же формат записи, как и в нашем датафрейме. То есть PandaSeries — это как раз-таки вот специальный формат хранения для различных типов данных, целых чисел, соответственно, дробных, даты, страны, текста. Но при этом сразу же из коробки у нас есть возможность с этими данными производить различные аналитические математические преобразования. Например, сразу посчитать медианку, среднее, или какое-то даже более сложное вычисление. Поэтому на простом языке, действительно, как Владимир написал, Pandas — это тот же Excel, только для большого количества данных. Это действительно так. То есть сейчас давайте сразу же посмотрим, сколько у нас данных хранится в нашем датафрейме. Обратите внимание, здесь 500 тысяч строк. Такой Excel, будучи открытым даже на сильном компьютере, начнет уже довольно сильно подвисать. И вот опять же в Pandas можно сделать все то же самое. Мы сегодня как раз это и проделаем. Только гораздо быстрее и в каком-то смысле гораздо элегантнее, потому что тот код, который вы один раз написали, он теперь навсегда останется с вами. А в Excel каждый раз все нужно делать, соответственно, еще раз. При этом, чтобы данные считать, обратите внимание, я прочитал их из архива, но на самом деле в этом архиве лежит обычный, самый обычный CSV-файл. И если мы напишем pandas.read, обратите внимание, что Pandas умеет читать в себя довольно большое количество форматов. То есть он может читать CSV-файлы, он может читать Excel-файлы, он может читать HTML-файлы, он может читать JSON-ы. Он может читать более аналитические форматы, которые вы бы узнали, если бы занимались BigData. Например, Parquet — это специальный формат данных, который хранит очень большие массивы. Соответственно, в Pandas можно считать SPSS-файл — это программа, которая занимается обсчетом статистики, или SQL-запросы, или таблички. В общем, практически все данные, которые так или иначе можно представить в виде текстовой таблицы, в CSV-файле, в обычном тексте или вообще в SQL-формате, можно без труда поместить в Pandas.",
        "1792eea0-8e31-4e0f-b072-c7acd6ee03d6": "Сумасшедший учении, Степик организации. Сумасштабированный интегратор. Призcíastick.ru Автор субтитров Karl Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Сумасштабированный интегратор Степик организации Степик организации Сумасштабированный интегратор Степик организации Друзья, всем привет! Рад видеть вас на нашем очередном открытом вебинаре по анализу данных Давайте пока подождем всех, кто подключается Если вам все прекрасно видно и слышно, ставьте, пожалуйста, плюсик в чат И буквально через пару минут начнем наше с вами погружение в аналитику Сегодня у нас будет немножко другая тема Мы уже довольно много говорили про статистику Сегодня немножко отвлечемся, поговорим больше про инструментальную сторону вопроса Поговорим про то, как можно анализировать данные в Pandas Это, пожалуй, одна из таких самых популярных, известных и широко используемых технологий для аналитики, для анализа данных, для Data Science Поэтому сегодня, я думаю, в первую очередь вебинарчик будет интересен тем, кто только начинает свое знакомство с анализом данных Мы пробежимся практически из самых таких базовых вещей Мы поговорим про самые простенькие вещи, но в конце заодно и обсудим еще и, соответственно, такие более чуть-чуть продвинутые детали Ну и что самое более важное, опять же, как и всегда, мы не просто будем акцентировать внимание на то, что такое Pandas и как там можно делать какие-то операции, грубая select from что-нибудь Скорее, опять же, возьмем определенный кейс, определенную аналитическую задачку и сразу пробежимся по тому, какие, собственно говоря, действительно есть функции в Pandas и как при помощи них можно сразу же решать какие-то аналитические задачи То есть совмещаем, как всегда, полезное с приятным и не просто учим какой-то синтезис Ну и сразу стараемся понимать, какие более важные навыки нужно иметь, чтобы анализировать данные Соответственно, буквально еще несколько минут и начинаем, пока как раз все подтягиваются Смотрите, во-первых, немножко по формату Мы сегодня решили провести вебинар на YouTube Соответственно, этот стрим вы можете видеть в реалтайме Вы можете писать все ваши вопросы, которые у вас возникают, в чатик Соответственно, какие-то вопросы я буду отвечать сразу Как, например, те, которые сейчас уже вы пишете Сколько длится вебинар? Я думаю, мы уложимся где-то в час, может, чуть меньше, может, чуть больше Соответственно, ссылка останется Ссылка останется, и вы можете посмотреть ее в нашей группе в ВК И уже не спеша повторить какие-то команды К сожалению, мы хотели-хотели сделать лайв-кодинг, чтобы у вас тоже получился доступ Прямо-невосредственно к питоновским скриптам, которые я буду писать К сожалению, я очень прошу прощения На этот раз мы не успели эту штуку провернуть Но обещаю, со следующего раза точно уже перейдем в формат лайв-кодинга Чтобы вы могли программировать вместе со мной И перед тем, как мы окончательно начнем, хочу принести еще одно небольшое извинение Мы случайно отправили сегодня вам сообщение о начале вебинара В таком очень низкоуровневом HTML-формате Самые продвинутые настоящие аналитики и программисты научились разумеется Конвертнули этот формат правильно Открыли его как отчетимое или все прочитали Прошу прощения, произошла небольшая техническая заминка Надеюсь, все-таки мы все здесь собрались И теперь сможем уже... Уже, соответственно, все получилось Тогда, я думаю, все, кто хотел к нам присоединиться, уже присоединились Я думаю, можем начинать Тогда давайте, в принципе, тогда начинать Я так понимаю, сейчас вам станет доступен не только стрим меня, но и видеоэкрана И, собственно говоря, будем уже решать наши первые задачи Так, ну что ж, как я уже сказал, сегодня мы поговорим про pandas Сегодня мы поговорим про чисто техническую сторону вопроса И, соответственно, вообще что это такое? Аналитика, анализ данных, data science Это, я думаю, для всех у вас уже понятная задача У нас есть какие-то данные, у нас есть какие-то запросы Очень часто такие запросы формулируются вообще не на языке какого-то такого очень понятного вопрос-ответ Сколько пользователей было в прошлом месяце Хотя такие вопросы тоже часто есть Иногда вопросы задаются очень на таком языке ресерча А как бы нам понять, что нужно сделать, чтобы у нас там подросли продажи? Ну, как нам понять, сколько пользователей у нас уходит, как сделать, чтобы они уходили меньше? Как понять, какие наибольшие категории для пользователей наиболее интересны и почему? Вот на такие вопросы, как что-нибудь понять, почему что-то происходит, мы привыкли общаться на русском языке То есть, понятное дело, что когда мы такие вопросы обсуждаем, продуктовые, мы не говорим на языке математики или Python Однако очень часто все-таки основной инструмент общения с данными для аналитика — это SQL SQL — подобный язык Все наши данные лежат в базах Когда у нас есть какие-то вопросы про то, что происходит, что что-то изменилось Это вопросы, очень часто которые можно перевести на язык запросов, структурированных запросов SQL И это, пожалуй, такая, наверное, главный такой хардхилл, который должен знать аналитик Уметь очень бегло говорить на языке SQL запросов с данными И второй, пожалуй, ну тут, наверное, не то чтобы кто-то больше, кто-то меньше Я сказал, два самых важных, в каком-то смысле одинаково важных — это Python Так уж сложилось исторически, что практически все аналитики, ну по крайней мере, которых я знаю, говорят на Python Еще не так давно, на самом деле, я когда сам вообще начинал погружение в анализ данных Я полностью все делал на R, такой тоже очень популярный язык программирования Но сейчас окончательно перебрался на Python И в Python Pandas — это как раз самая популярная история, самая популярная библиотека Которая позволяет нам с данными как-то работать, манипулировать И вот именно сегодня про нее и поговорим В общем, небольшое введение Сегодня говорим про Pandas Весь скрипт, который я сегодня буду делать, будет вам тоже доступен после нашего вебинара Я выложу его в группу нашей Вконтакте Соответственно, все данные, с которыми я работаю, тоже выложу Соответственно, сегодня можно просто как бы за мной наблюдать, посмотреть Позадавать какие-то вопросы, которые у вас есть И, соответственно, вместе со мной пробежаться по такой еще одной аналитической задаче, которую мы разбираем в real-time Ну что ж, тогда какой у нас план сегодня на вебинар?",
        "af2ad4ec-db25-49e0-b36f-3d645fb55f5c": "В прошлом занятии мы выяснили, что у нас за продукт, какие данные мы собираем и как мы организовали их хранение. Теперь пришло время попробовать на эти данные уже посмотреть. Давайте сейчас вместе откроем Redash и сначала разберемся с тем, что это такое. Redash это как раз таки инструмент, который позволяет нам взаимодействовать с нашими базами данных, писать различные запросы к ним, визуализировать какие-то результаты и удобно работать с данными, которые у нас хранятся. То есть Redash это один из таких наиболее популярных инструментов, которые используют аналитики при работе с базами данных. Хотя это не единственный инструмент, их довольно много разных, но вот Redash довольно популярный. Итак, когда мы попали на стартовое окно, нажимаем Create, New Query. Обратите внимание, что помимо запросов есть еще и дашборды, алерты, об этом поговорим чуть позже. И когда мы нажали New Query, здесь наверху мы должны выбрать нужную нам базу данных. В данном случае у меня есть чуть больше доступов, у вас это симулятор SQL. И вот мы сейчас оказались в симуляторе SQL, у нас тут слева такой слоник нарисован, а вот внизу вы видите уже набор знакомых нам табличек, про которые мы поговорили в прошлом уроке. Теперь пришло время сделать, как говорится, важное объявление. Итак, SQL это просто язык, на котором мы общаемся с базами данных. Он так и переводится – Structured Query Language. То есть некоторый язык, команд, который позволяет нам говорить, что мы хотим из баз данных достать и в каком формате. А база данных – это хранилище наших данных, та коробка, в которую мы кладем нужные нам таблицы. Причем таблицы – это просто как нам удобнее так представлять. Понятное дело, что в самой базе данных данные могут храниться куда более сложным образом. Теперь, что за слоник у нас нарисован? Слоник – это как раз таки база данных, это логотип, который использует PostgreSQL. Вот PostgreSQL – это уже непосредственно база данных, которая хранит в себе нашу информацию про курьеров, пользователей и так далее. И поэтому здесь надо немножко не путать эти вещи. Я понимаю, что ребята, которые уже в курсе, наверное, это все скучно слушать, но те, кто только начинают, давайте еще раз проговорим. PostgreSQL – это хранилище данных. SQL – это язык запросов, при помощи которых мы можем обращаться к нашей базе данных. Ну, давайте напишем первый, собственно говоря, запрос на языке SQL. Я думаю, вы даже если никогда в жизни не слышали ничего про SQL, уж точно слышали что-нибудь про слово SELECT**FROM в словосочетании. И это действительно валидная запись на языке SQL. SELECT** означает «достать вообще все» – FROM IS, и дальше должно идти название таблицы. В рядаше, кстати, мы можем нажать вот так вот на эту вот стрелочку, и название таблицы сразу напечатается у нас в окне для запроса. И в конце напишем LIMIT 10. Нажмем EXECUTE, и внизу вы увидите результат нашего запроса. Вот, это как раз та структура данных, про которую я говорил в прошлом занятии. У нас есть id-шник курьера, у нас есть id-шник заказа, который этот курьер что-то с ним сделал. И вот что он с ним сделал, мы видим в табличке ACTION. Сначала он его принял, потом он его доставил. И, соответственно, вот мы видим, что у нас еще здесь есть время, когда это все произошло. Теперь давайте чуть подробнее остановимся на этой записи. Значит, смотрите. SELECT означает, что мы хотим что-то из таблицы взять. Что мы хотим взять? Мы хотим взять звездочку. FROM – это из какой таблицы? Вот из этой вот. А LIMIT 10 означает, что мы хотим взять только первые 10 строчек. Обратите внимание, у нас вот здесь написано, мы получили только 10 строчек, которые достались из этой базы. Еще раз, звездочка означает взять вообще все колонки. При этом мы, разумеется, можем написать более явно. Например, SELECT ACTION, id-шник курьера, заказ, time, FROM таблицечка, курьер ACTIONS, LIMIT 10. И вот такой запрос мы можем исполнить. И, соответственно, получим тот же самый результат. Мы как бы 10 строчек достанем из нашей таблицы. Ну что ж, поздравляю, вы написали ваш первый SQL-запрос. Мы можем здесь поставить какие-то другие таблички. Например, посмотреть, что у нас хранится в табличке USERS ACTIONS. И увидим, что, в принципе, прошу прощения, есть немножко другие колонки. Только, разумеется, у нас будет ACTION, ORDER ID. А вот здесь у нас будет не КУРИЕР ID, а USER ID. И вот мы получим 10 строчек из нашей таблицы USER ACTIONS. И, соответственно, вот здесь вы видите, как работает самый простой SQL-запрос. Еще раз давайте по нему аккуратненько пройдемся. Видите, Redash подсвечивает ключевые слова SELECT, FROM, LIMIT. Это означает, что это как раз-таки команды именно SQL-ные. То есть это такие ключевые слова, после которых ожидаются конкретные вещи. Например, после SELECT ожидается либо список колонок через запятую, либо звездочка, если мы хотим все колонки достать. А после FROM ожидается название таблички. После LIMIT ожидается набор строчек, которым мы хотим ограничить наш вывод. И сразу же скажу, что таблицы могут быть огромные, в них могут храниться миллионы данных. Если вы просто напишите SELECT звездочка FROM TABLE, нажмете Выполнить, то, возможно, ваш запрос даже не станет выполняться на сервере, потому что умные дата-инженеры такие запросы запрещают делать аналитикам. Либо будет выполняться очень долго, и это не очень хорошо, потому что зачем нам все эти десятки миллионов данных пытаться из базы данных вытащить к нам сюда в Redash зачем-то? Это такой плохой запрос, так не делайте. Поэтому всегда, когда вы хотите использовать SQL-запрос, чтобы просто посмотреть, что у нас там за данные хранятся в нашей таблице, то используйте вот такую вот запись SELECT нужные вам колонки FROM таблица и какой-то LIMIT. Пару слов еще буквально про то, что здесь у нас, я вот писал два варианта, либо таблички через запятую, строчки через запятую, прошу прощения, колонки через запятую, либо звездочка для того, чтобы написать как бы SELECT все. Ну и здесь тоже скажу, что вот SELECT звездочка – это немножко такая хулиганская запись. Это можно, конечно, делать, если вы так быстренько как-то делаете такой эксплуаторный анализ ваших данных, но лучше всегда писать более явно. То есть чем более явный запрос вы написали, тем лучше. То есть вот такой вот запрос, когда у нас есть перечисление конкретных колонок, которые мы хотим вытащить, это даже более правильно с точки зрения читаемости нашего запроса. И другим аналитикам будет гораздо понятнее с ним работать, потому что они сразу понимают, что именно вы из таблицы достаете и зачем. То есть вот такой запрос у нас может получиться. Обратите внимание, я еще здесь использовал сочетание клавиш COMMAND ENTER. И, соответственно, это тоже можно исполнять запросы, не нажимая кнопочку EXECUTE. И вот здесь я вам сейчас расскажу последнюю ключевую фразу, которую мы еще можем добавить к нашему запросу. Это фраза, которая называется ORDER BY. И это просто сортировка наших данных. Например, мы можем отсортировать все наши данные по, скажем, user id, по значению колонки user id, добавить, допустим, DESK. Это у нас будет сортировка по, соответственно, убыванию, самый большой id пользователя, и дальше поменьше, поменьше, поменьше. Если мы DESK уберем, то это будет, наоборот, по возрастанию.",
        "b6f8c5bf-111e-4edd-9fa2-829abe15fce3": "Итак, мы создали две переменных. В каком-то смысле получилась уже маленькая история. У нас есть аналист Анатолий, который учится два года. Теперь с этими переменными можно поработать. Ну, под словом «поработать» в каком-то смысле я понимаю, с ними что-то сделать. Мы уже что-то делали. Мы просто эти переменные выводили в output или выводили на печать при помощи функции print. Но пока как-то выглядит не очень впечатляюще. Что еще можно сделать с переменными? Ну, смотрите. Во-первых, если у нас yers – это целое число, то мы можем делать с ним все, что мы можем делать с числами. Например, складывать с другими числами, умножать то, что мы уже делали. Или, допустим, мы можем создавать другие переменные, используя нашу старую переменную. Но, что более важно, что еще можно сделать с числами? С числами еще можно сравнивать друг с другом. Например, правда ли, что Анатолий учится больше пяти лет? И в этом смысле мы получим false, потому что двойка меньше пяти. А если бы мы спросили, правда ли, что Анатолий учится меньше пяти лет, мы получили бы true. И вот этот false и true, который мы получаем при сравнении переменных, это не просто текст – false или true. Это именно специальный тип данных в Питоне, который называется логическим типом, который может иметь только два состояния – либо false, либо true. С точки зрения аналитики, этот тип данных нам тоже будет довольно часто быть полезен. И, естественно, это особенно часто будет проявляться тогда, когда мы делаем какие-то запросы к нашим данным. Например, запросы в базы данных или запросы в наши данные, с которыми мы работаем в Питоне. Допустим, нам нужно будет отобрать всех тех аналистов, у которых опыт работы меньше пяти. И вот именно тип данных true-false здесь нам как раз-таки и потребуется. Что еще интересного можно сказать про этот тип данных? Ну, во-первых, как бы мы могли спросить, оправдывали ли, что Анатолий учится ровно два года. И вот здесь мы могли бы по неопытности написать yers=\"2\", но, обратите внимание, это было бы ровно то же самое, что написано выше. Это просто было бы создание переменной. Потому что знак равенства, как я сказал, в Питоне используется не для проверки на то, что левая часть равна правой части, а на присвоение. То, что правая часть присваивается как бы значение справа, теперь присваивается к переменной слева. И все просто. Для того, чтобы убедиться, что одна переменная такая же, как другая, допустим, что yers=\"2\", используется два знака равно. Обратите внимание, что здесь также мы получаем true по аналогии, если бы мы сказали, что yers=\"-5\". То есть получить true можно принципиально разными способами. Что еще важного? Мы можем сказать, что yers=\"2\", это тоже будет true. yers=\"-2\", это тоже будет true. Ну и для разнообразия yers=\"-2\", это будет false, по понятным причинам. Поэтому давайте создадим еще одну переменную, допустим, hasJob, и она будет равняться false. И вот теперь мы рассказали целую небольшую историю про аналитика Анатолия. Что его зовут Анатолий, что он учился два года, и то, что hasJob у него false. Соответственно, работы он пока не получил. Это довольно удобно. Теперь мы можем с этими переменными как-то работать. Мы можем, как я сказал, проверять небольшие, маленькие вопросы про нашего аналитика. Например, спрашивать, сколько он уже учился, или есть ли у него работа, или как его зовут. Но пока что с этими переменными ничего особо сильно не поделаешь. И аналитик, тем более, у нас только один. Если бы мы хотели сохранить несколько аналитиков, что бы делали мы тогда? Ну, смотрите, во-первых, мы могли бы просто создать еще одного аналитика. Например, мы могли бы сказать, что есть еще аналист 2, пусть это будет Ольга. И, допустим, она обучается уже аналитике 3 года, и hasJob, допустим, у нее уже true. Обратите внимание, что для того, чтобы создать нового аналитика, я использовал новые названия переменных. То есть добавил двоечку через нижнее подчеркивание к названию переменной. Это довольно важно, потому что если бы мы написали, например, вот так вот, то есть просто хотели бы поставить пробел, то здесь бы наш питон не понял. Обратите внимание, мы бы столкнулись с первой синтаксической ошибкой. Иными словами, мы сказали на языке Python какое-то выражение, которое грамматически неправильно сформулировано. Это как если бы мы написали бы русское предложение на бумаге, поставили точку в ассоциативный знак и снова продолжили писать с маленькой буквы. То есть такая комбинация знаков припинания и заглавных и строчных букв просто не является правильным высказыванием на русском языке. То же самое, что аналист пробел 2 равно Ольга тоже не является никаким осмысленным высказыванием на питоне. Если мы хотим включить двоечку в название переменной, мы можем написать либо без пробела анализ 2, либо использовать вот такое вот нижнее подчеркивание. В программировании очень часто это используется как раз для создания переменных. Обратите внимание, я такой приемчик уже использовал чуть выше, соответственно, когда делал переменную hasJob. Вот теперь у нас есть две переменные. Ну и, например, опять же, мы можем уже с ними теперь проводить какие-то операции. Допустим, узнать, какой из аналитиков обучался аналитике больше. Для этого можем сравнить, допустим, yers и yers2, получить false. Это означает, что yers2 у нас больше либо равен, чем yers. Отлично, но обратите внимание, что такой подход для работы с переменными, наверное, не очень удачный, потому что если бы у нас было 5 аналитиков, 6 аналитиков, нам нужно было бы от каждого из них заводить какую-то свою переменную, как-то их называть, анализ 1, анализ 2, анализ 3. В принципе, довольно легко запутаться. Я думаю, даже сейчас вы понимаете, что тот тип названий, которые я сейчас показываю вам, в каком-то смысле не является наиболее эффективным способом названия переменных в коде, потому что, по сути, он просто как бы дублирует одни и те же самые переменные с некоторым уточнением. 1, 2, 3, 4, 5, и в таких переменных очень легко запутаться и забыть, кто у нас аналитик Анатолий, а кто аналитик Ольга. Поэтому давайте научимся сохранять сразу несколько объектов в одну переменную. Давайте заведем переменную анализ, вот так вот. И здесь сохраним сразу два аналитика. Сохраним и Анатолия, и Ольга. Даже если вы никогда раньше не программировали, я думаю, что сейчас вы понимаете. Тут чего-то не хватает, не правда ли? Не хватает какого-то такого элемента синтексиса, который показал бы нам, что Анатолий и Ольга теперь лежат, хранятся вместе. Давайте для этого поставим вот такие квадратные скобочки. Нажмем Shift-Enter или Run и выведем на печать получившийся результат. И теперь внимательно посмотрим на эту запись. Это уже чуть более сложное высказывание, которое может быть прочитано следующим образом. В переменную analysts мы сохранили список из двух аналитиков. Список в питоне — это как раз вот такая структура данных, которая позволяет хранить какой-то набор элементов. Ну, условно, например, набор из нескольких строк. В данном случае это строка Анатолий и Ольга. Также мы могли бы сохранить список, в котором сохранили бы число лет.",
        "2779a745-e768-4170-96de-107967d4d5b2": "Всем привет, это снова Анатолий Карпов, мы продолжаем наши занятия по питону для анализа данных, и на втором занятии мы уже, собственно говоря, займемся анализом данных. Первое занятие было по питону, второе будет по Python анализов данных. Мы сейчас освоили самые базовые понятия, как создавать переменные, как работать с переменными, складывать их друг к другу, умножать, вставлять значения переменных в циклы, вставлять значения переменных в if и else. На самом деле, это было такое очень короткое введение, но хорошее новое заключается в том, что этого пока что нам более чем достаточно, чтобы перейти уже к следующей теме, чтобы заняться уже именно питоном, который позволяет нам решать аналитические задачи. Поясню почему, потому что, как я говорил, в питоне есть специальные библиотеки для работы с данными. Еще не так давно, до возникновения таких библиотек, когда программисты хотели решить какую-нибудь аналитическую задачу, они должны были писать целый такой код, писать какие-то скрипты, как-то эти данные хитро сохранять в списке многомерные массивы, словари и так далее. Но времена эти прошли, и сейчас в питоне есть специальные библиотеки, которые именно разработаны для того, чтобы работать с данными, с данными, которые чаще всего представлены в табличном виде, и большинство аналитических функций, запросов уже как бы записано за нас, нам их нужно только выполнить. И в этом смысле владение таким питоном можно сравнить с управлением автомобилем, то есть вам не нужно знать досконально, как устроен Tesla автомобиль, чтобы сесть в него и поехать. То же самое и с питоном. И редко когда от вас как от аналитика будут ожидать production level программирования, а именно как разработчика. Поэтому то, что мы делаем, это не то, чтобы мы как-то специально там очень поверхностно знакомимся с материалом, наоборот, мы знакомимся просто ровно с тем материалом, который нам нужен. И вот это вот прикладное такое аналитическое программирование, используя библиотеки данных, ровно то, что нам потребуется. Итак, смотрите, я создал еще одну папочку себе, переименовал юпитер ноутбук в lesson 2, и соответственно здесь мы будем работать на нашем втором занятии. Начнем мы наш скрипт с вот такой команды, мы напишем import pandas spd. Ну вы видите, что скрипт отработал без ошибки, давайте сразу разберемся, что же здесь произошло. Pandas это такая очень известная библиотека для работы с данными, очень похожая на панды, но нет, pandas называется потому, что это panel data. Panel data это как раз вот та структура данных, которая нам максимально всем знакома, когда у нас в строках какие-то наблюдения, а в колонках название значений. Допустим, там в строках у нас список аналитиков, список клиентов, а в колонках там имя, фамилия, год рождения и там прибыль с этого клиента. Соответственно, pandas это название библиотеки, в которой есть множество функций. Самое первое из них будет называться pd.read.csv, то есть первым делом мы считаем файлы. И вот обратите внимание, сейчас мы записываем это название сокращенно, просто pd, вот потому, что мы нашу библиотеку pandas импортнули, призвали как сокращенное название pd. В этом нет никакого сакрального смысла, можно было импорт pandas s, что угодно, просто импорт pandas s pd, это такая уже общепринятая нотация, которая используется в мире анализа данных. И в большинстве примеров кода, которые используют библиотеку pandas, именно pd будет использовано в качестве такого сокращения. Дальше мы переходим к задаче считывания внешнего файла, который лежит в csv. Это наша сегодня будет первая аналитическая задача. У нас будут данные по продаже некоторого продукта, и вот мы как аналитик должны будем сделать такой разведывательный анализ данных. Обратите внимание, что когда мы начинаем считывать данные, мы видим, что pandas предоставляет множество возможностей считать внешний формат, не только csv, но и xlx файл, html файл, json файл, то есть в принципе существует довольно много форматов данных, и большинство из них можно считать в pandas прямо вот так сказать из коробки. Нам потребуется pd.read.csv. pandas.read.csv это функция, которая получает на вход путь к файлу, и соответственно считывает его в такой так называемой pandas dataframe. Обратите внимание, что вот в lesson 2 я положил файлик lesson1.data.csv. Он также хранится в папочке с общим доступом, и вы можете считать его оттуда. Что для этого нужно сделать? Смотрите, pd.read.csv, если мы нажмем tab, нам сразу, не так-то же, если мы начнем писать путь к файлу, то мы увидим, что вот нам Python предлагает продолжить путь к файлу. Обратите внимание, мы можем выбрать сейчас lesson2, мою директорию, и здесь увидеть тоже есть разные файлы. Вот какие-то из них, это там название юпитер ноутбуков и служебные файлы, а вот этот вот lesson1.data.csv это тот файл, который нам нужен. Для тех, кто не знаком с линуксом и командной строкой, обратите внимание, что как бы путь к файлу мы записываем, как такое расположение его в нашей файловой системе. И на самом деле вот мы сейчас находимся в моей папке home.yupiter.uncarp.lesson2 и вот соответственно csv файл. Вы можете считать этот файлик из директории shared, ссылочку на считывание я приложил перед этим уроком. И как вы понимаете, этот файлик мы хотим считать не просто так, а в какую-то директорию, прошу прощения, в какую-то переменную. Ну давайте для простоты назовем переменную df. Что же за файл, с которым мы будем работать? Это файлик, в котором вот есть информация про продажи различных курсов. Вы видите, что здесь есть номер заказа, дата заказа создания, дата оплаты, название курса, статус, сколько мы заработали на нем, из какого города был покупатель и соответственно какой системой оплаты он пользовался. Ну вот это такая классическая история. Файлик сохранен у нас как бы получается в такой формате вот такого panel data и соответственно нам ничего не мешает его теперь перенести в Python. Для этого мы воспользуемся функцией pandas.read.csv. Вот мы передали наш с вами файлик в качество такого путя и соответственно здесь выполняем команду. Но не все так просто, обратите внимание, сразу же натыкаемся на какую-то ошибку. Нам пишут, что UTF-энкодинг не смог справиться с кодировкой. Это нормально, файлы текста бывают разной кодировки и соответственно очень часто нужно дополнительно указать энкодинг и в данном случае это будет Windows 12.51. Почему Windows 12.51? Ну потому что в большинстве случаев либо UTF-8, либо Windows 12.51. Это хорошая новость. Ну и еще довольно интересная ошибка. Снова ничего не помогло. Мы видим, что все еще мы получили ошибку считывания. Где-то вот там говорят, что мы ожидали одно поле, а увидели два. Что это значит? Дело в том, что в русскоязычной реальности csv файлы не совсем csv файлы. csv файл шифровывается как comma-separated values, но в связи с тем, что у нас запятая используется для разделения десятичных разрядов в десятичной дроби, то csv файлы чаще всего у нас сокращаются, сохраняются с разделителем, который является точкой запятой. Если бы мы теперь указали эту дополнительную настроечку, то у нас все получилось. Опять же у вас может возникнуть законный вопрос. Анатолий, ну тебе хорошо, ты помнишь вот эти вот подводные камни наизусть, знаешь, что есть переменная argument-encoding, знаешь, что есть переменная separator, аргумент-сепаратор, который надо указать точкой запятой, но как бы мы могли дать туда кида самостоятельно?"
    },
    "relevant_docs": {
        "ad7fd8fe-f70b-4fd2-ba1c-c8ef4765272c": [
            "489beae3-401b-4c19-bef8-8e09c99f3390"
        ],
        "a1630906-79d8-40dd-a2fc-36804caf170b": [
            "db5f2d5d-f989-4504-940a-54aca7ca37eb"
        ],
        "6481f74c-887f-47ee-b46c-f833e86200b2": [
            "740ab60f-f15f-432b-b8a3-8652b73e8508"
        ],
        "1feb73cc-3c39-4746-9085-eec0b010284e": [
            "fd55a8da-f7b9-46e7-ad8f-26e9b243948e"
        ],
        "12662d67-26b0-44dd-980f-8a5e31ce99d7": [
            "46608081-7a4e-493a-b5b9-9205d1645cbf"
        ],
        "0e7673dc-2c78-43b4-a5a4-4b1937830aa2": [
            "1554b1d9-a57b-4508-9536-d388def0ebe1"
        ],
        "fec89b93-6f98-4689-aea9-8a99c38e109d": [
            "1792eea0-8e31-4e0f-b072-c7acd6ee03d6"
        ],
        "bd103bde-bb62-475e-8444-1c79dfaf44e0": [
            "af2ad4ec-db25-49e0-b36f-3d645fb55f5c"
        ],
        "7053df67-e626-46ee-b70c-fd47b3f9fba8": [
            "b6f8c5bf-111e-4edd-9fa2-829abe15fce3"
        ],
        "6ce6aae0-299d-42ee-9bc2-70b85e767e20": [
            "2779a745-e768-4170-96de-107967d4d5b2"
        ]
    },
    "mode": "text"
}