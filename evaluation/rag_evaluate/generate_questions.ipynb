{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node sampler - sample more relevant (informative) nodes \n",
    "# qagenerator - generate questions to the sampled nodes\n",
    "from evaluate_rag_utils import NodeSampler, QAGenerator\n",
    "\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more relevant nodes\n",
    "# having higher counts of relevant and lower count of irrelevant words\n",
    "# limit sample to 5% of all nodes\n",
    "node_sampler = NodeSampler('relevant_words.txt', 'irrelevant_words.txt')\n",
    "informative_nodes = node_sampler.informative_nodes(video_info_path='../../data/video_info copy.json', fraction=0.05)\n",
    "\n",
    "nodes = informative_nodes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# create QA generator\n",
    "qa_generator = QAGenerator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate:\n",
    "# i) question-node pairs, save as json at qa_json_path\n",
    "# ii) question-document pairs at doc_question_path\n",
    "qa_dataset = qa_generator.generate_document_question_pairs(\n",
    "    nodes=nodes,\n",
    "    doc_json_path='../../data/video_info copy.json',\n",
    "    doc_question_path='doc_question_dataset.json',\n",
    "    qa_json_path='question_node_dataset.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queries', {'431ae521-f3cc-4508-8d1e-d4cca770c01d': '\"Какие навыки и качества важны для работы в анализе данных?\"', '983cc25f-494b-4e4a-9743-ffe977a71aaf': '\"Какие основные различия между ML-инженером и Data Scientist?\"', '3a18b2a8-a432-4104-9254-a7ccce061fde': '\"Какие основные компетенции и технический стек у Data Analytics?\"', 'caa9b9f5-615b-49e0-ac22-e633280e97d0': '\"Какие hard skills необходимы для работы аналитиком данных?\"', '020cd976-1670-43c5-99cc-2a3f10295841': 'Какой алгоритм используется в градиентном бустинге для прогнозирования?', '3faea57e-9f5c-453b-8e86-a2c1b0913ffd': '\"Какие навыки ценятся на рынке труда в области анализа данных?\"', 'd54de249-9956-4f9a-9d50-d478fb9792f0': '\"Что такое Redash и как он помогает аналитикам работать с данными?\"', '78a445b7-d793-41f6-b884-1a6467be8080': '\"Какие hard и soft skills нужны начинающему аналитику данных?\"', 'a945cb2e-feb0-44c3-8b5f-3457f74b450b': '\"Какие задачи можно решать при помощи оконных функций?\"', '81f5cbd3-f999-450b-9300-7f5d9dfee352': '\"Какие проблемы возникают при хранении данных в гугл-документе?\"'})\n",
      "('corpus', {'42682ca2-1051-4d31-8964-7e941bbaca28': 'Должны быть навыки презентации, умение доносить результат своей работы руководителю, то есть руководитель вам поставил задачу, вы должны обосновать, что вы получили, какие выводы и какие рекомендации, не знаю, какие гипотезы вы проверяли, это важно. Критичность. Готовность задавать много вопросов себе и окружающим. Это очень ценное качество для джина,\\nМашинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение, Машинное обучение,\\nТехническое интервью без тестовых заданий Расширяйте нетворкинг Третий совет.', '3ef1626e-080e-44f0-a84e-d5d023c0c5d6': 'По сути, ML-инженер — это Python-разработчик, который обладает специфическими знаниями в области данных. Например, он использует классический DSTack — это Python, NumPy, SocketLearn или SciPy. В отличие от Data Analytics, ML-инженер не строит статистические гипотезы, он их уже проверяет. При этом в проблемы бизнеса он может быть включен достаточно поверхностно. И теперь пришло время поговорить о Data Scientist. Казалось бы, любого специалиста в области Data Science можно назвать Data Scientist, но как-то так сложилось, что Data Scientist принято называть специалистом, который работает на стыке Data Analytics и ML-инженерии. Data Scientist — это человек, который хорошо пронимает проблемы бизнеса, но при этом уже умеет строить более сложные ML-решения и на основе этого помогает бизнесу отвечать на его вопросы. Можно сказать, что Data Scientist — это аналитик с базовым знанием ML-инжиниринга. Так же, как Data Analytics, он строит гипотезы, их проверяет и помогает интерпретировать результаты для бизнеса, чтобы помочь получить ответы на какие-то вопросы. При этом простые модели Data Scientist может обучать сам, но иногда ему также нужна помощь ML-инженера. Напоследок хочется сказать, что мое, либо какое-либо другое разделение специальностей в области Data Science достаточно условно. Оно может различаться в зависимости от компании к компании, от страны к стране, а в маленьком стартапе отсутствовать вовсе. Кроме того, нельзя забывать про множество подвидов Data специальностей. Например, это могут быть BI-аналитики, продуктовые аналитики, разработчики алгоритмов нейронных сетей. Отсюда следует, что при поиске работы внимательно читайте описание вакансии, не бойтесь задавать уточняющие вопросы, попросите рассказать вам о стеке и конкретно о задачах, которые вам нужно будет решать. Также помните, что никогда не поздно сменить траекторию развития, так как знания, полученные в одной области Data Science, могут быть вам полезны и в другой.', '0e9b7684-d754-49d4-b3a9-59bb55a62723': 'Дальше мы поговорим о Data Analytics. Этот специалист как раз очень сильно взаимодействует с бизнесом, потому что его задача как раз отвечать на вопросы клиентов. Как новая рекламная кампания влияет на продажи или почему от нас уходят наши клиенты? Аналитик как раз отвечает на эти вопросы исходя из данных, а не полагаясь на свое какое-то жизненное понимание или интуицию. Основные компетенции Data Analytics это построение статических гипотез, их проверка, проведение АБ-тестов, визуализация, построение дэшбордов и наглядная визуализация результатов для бизнеса. Такой специалист должен иметь достаточно сильную математическую базу, в отличие, например, от Data Engineers, поскольку ему приходится работать со статическими гипотезами и проверять их. Технический стек такого специалиста достаточно обширен и может варьировать в зависимости от доменной области, сложности задач или опыта самого специалиста. Сегодняшний стек такого специалиста составляет Python, SQL и биосистемы наподобие Tableau. Реже могут использоваться простые ML-алгоритмы типа линейных или логистических регрессий, также язык R и совсем крайне в случаях Excel. Дальше мы поговорим про ML-инженера. Задачи бизнеса могут быть достаточно сложными, когда умений Data Analytics уже не хватает, тогда приходится строить более сложные системы. Этим как раз занимается ML-инженер. Роль ML-инженера в команде — это как раз решать специфические технические задачи, которые ставят ему его коллеги-аналитики. Если просто аналитик пользуется какими-то простыми ML-алгоритмами типа регрессий, то ML-инженер строит уже более сложные решения на основе деревьев решений, кластеризации или композиции этих алгоритмов. Сфера его ответственности — это как раз построение моделей, их обучение, развертывание и поддержка в продакшене. По сути, ML-инженер — это Python-разработчик, который обладает специфическими знаниями в области данных. Например, он использует классический DSTack — это Python, NumPy, SocketLearn или SciPy. В отличие от Data Analytics, ML-инженер не строит статистические гипотезы, он их уже проверяет. При этом в проблемы бизнеса он может быть включен достаточно поверхностно. И теперь пришло время поговорить о Data Scientist.', 'f8173775-a0ce-42c1-adb5-7682c4e2fb68': 'SQL — это на самом деле тоже джентльменский набор, без этого никуда. Как я сказал, что если в 90% случаев в резюме от вас будут ожидать, что вы знаете питон, то вот в 100% случаев от вас будут ожидать, что вы знаете SQL. База данных. База данных — это основной способ хранить данные в индустрии. И на самом деле есть огромное количество различных баз данных. Есть там, вы слышали, наверное, MySQL, Postgres, Clickhouse, Vertica. Список можно продолжать довольно долго, но это, пожалуй, самые популярные. И все они объединены тем, что есть некоторый такой общий, так скажем, подход, который позволяет нам взаимодействовать с информацией, которая хранится в этих базах данных. Извлекать эту информацию, как-то трансформировать, этот подход называется SQL. Это некоторый такой целый набор команд, инструкций, при помощи которых вы научитесь извлекать данные из баз данных, работать с ними. И, в принципе, очень часто это выглядит как раз-таки такой совместной командной работой. Вы при помощи SQL забираете данные из баз данных и после этого, например, анализируете их на питоне. То есть помимо того, что все модули, которые в этой программе включены, они довольно важны для того, чтобы стать хорошим аналитиком, мы еще и постарались очень явно показать некую преемственность этих модулей, что они не разрозненные какие-то между собой, а они все в целом создают единый такой набор инструментов, которые крайне необходимы аналитику. И SQL, пожалуй, я даже не знаю, тяжело сказать, кто сильнее, питон или SQL, но, в общем, и питон, и SQL – это такой basic hard skills, которые просто необходимо иметь любому человеку, который хочет заниматься анализом данных на сегодняшний день. Разобравшись с такими тяжелыми hard skills, как питон и SQL, мы переходим к не менее интересной теме, которая называется введение в теорию вероятностей. И это как раз-таки показывает тот факт, что когда вы хотите стать аналитиком данных, вы должны понимать, что аналитик данных – это действительно, ну, в своем роде такой человек-оркестр, то есть вы должны уметь и программировать, и работать с базами данных, и понимать бизнес, и понимать какие-то математические основы.', '1907c3bc-201f-4dbf-876e-106db1911278': 'Ребята, всем привет! Меня зовут Григорий Бударагин, я аналитик-разработчик компании Яндекс. Сегодня я расскажу вам механизм работы градиентного бустинга. Что же такое градиентный бустинг? Это алгоритм машинного обучения, который помогает прогнозировать различные числовые величины. Например, он используется в различных больших технологических компаниях для ранжирования веб-страниц, для выставления цен в приложении такси, а также для противодействия мошенничеству и многого другого. Не только градиентный бустинг, но и другие алгоритмы машинного обучения учатся прогнозировать числовые величины на каких-то исторических наблюдениях. Например, чтобы научить алгоритм предсказывать цену дома, ему необходимо наблюдение о проданных в прошлом домах с их ценами и другими измеряемыми характеристиками. В качестве характеристик подойдут, например, количество комнат в доме или, возможно, его общая площадь. Также ни для кого не секрет, что большие дома в среднем стоят дороже, чем маленькие. Так вот, алгоритмы машинного обучения видят эту и другие закономерности в данных и подбирают для новых домов соответствующие цены. Что важно при этом, чтобы спрогнозировать цену нового дома, алгоритму необходимо иметь те же самые измеряемые характеристики, что и для исторических наблюдений для проданных в прошлом домов. Мы прояснили, как работает машинное обучение в принципе, но давайте вернемся к градиентному бустингу. У этого алгоритма есть одно очень важное свойство. Он объединяет в себе множество так называемых слабых алгоритмов. Так вот, в нашем случае слабый алгоритм – это дерево решений. И градиентный бустинг, в свою очередь, комбинирует множество деревьев решений, чтобы улучшить точность прогнозирования. При этом, каждое новое дерево решений – это самостоятельный алгоритм машинного обучения, которому нужны все те же данные, что и сильному алгоритму. Более того, каждое дерево способно давать прогноз самостоятельно. Поэтому для понимания полной картины градиентного бустинга необходимо понять, как работают деревья решений.', '95a28af4-63f2-4d95-a90c-6795746569bf': 'Возможно, для вас это будет определенный такой шаг, как войти в новую профессию. Продуктовых аналитиков, если мы говорим, достаточно немного, то есть 426 вакансий, остальных аналитиков ВВА и прочих достаточно узких сегментов стало еще меньше. Что мы говорим про количество откликов на вакансию? Вы можете посмотреть, что если мы берем стажерские позиции, что количество откликов на вакансию, например, без опыта людей где-то 428, о чем это говорит? Чтобы вы понимали, что если, например, какая-то компания X публиковала вакансию, то уже с людей без опыта откликнулся 428. Конкуренция очень высокая. Мы даже смотрели компанию Megafon, там количество откликов более 775, если я не ошибаюсь, не изменяет память. То есть здесь нужно суметь выделиться среди своих конкурентов, суметь прокачать свое резюме так, чтобы оно работало. Если мы говорим про другие позиции, например, аналитик данных без опыта, количество откликов в среднем 167, Data Science 127. И вы видите, чем выше опыт, тем количество откликов на вакансию меньше. Если мы возьмем, например, инженер данных, там 3-6 лет опыта, то получается 5 откликов на вакансию. То есть практически минимальная конкуренция. Меня часто спрашивают ребята, например, а что мне нужно прокачивать? А какие сейчас харды ценятся на рынке труда? Если мы говорим про аналитиков данных, то максимальный акцент на SQL и Python. Вы видите по показателям, что SQL требуется в более 100 вакансий. Наверное, где-то 102-103. Там SQL и требования чуть ли не подчеркнуты с отлицательным знаком. Поэтому, если вы собираетесь развиваться в мире анализа данных, качаем SQL. Для этого есть специальные сайты, где вы можете потренироваться, прокачивать себе этот скилл. Это все в открытом доступе. То же самое Python на втором месте. Анализ данных на третьем табло и дальше по убыванию. Если мы говорим про специализацию инженера данных. Что здесь? То же самое SQL, он никуда не делался. Python на втором месте. Дальше у нас идет ARL, Hadoop, Spark. Вы можете ознакомиться с SQL, он ведущий. Ситуация немножко меняется, если мы говорим про DS, про ML. То здесь вперед выходит Python. То есть он практически must-have. Дальше на втором месте идет SQL.', '6e76feb9-af66-4c7b-bdc9-f26666c0d063': 'В прошлом занятии мы выяснили, что у нас за продукт, какие данные мы собираем и как мы организовали их хранение. Теперь пришло время попробовать на эти данные уже посмотреть. Давайте сейчас вместе откроем Redash и сначала разберемся с тем, что это такое. Redash это как раз таки инструмент, который позволяет нам взаимодействовать с нашими базами данных, писать различные запросы к ним, визуализировать какие-то результаты и удобно работать с данными, которые у нас хранятся. То есть Redash это один из таких наиболее популярных инструментов, которые используют аналитики при работе с базами данных. Хотя это не единственный инструмент, их довольно много разных, но вот Redash довольно популярный. Итак, когда мы попали на стартовое окно, нажимаем Create, New Query. Обратите внимание, что помимо запросов есть еще и дашборды, алерты, об этом поговорим чуть позже. И когда мы нажали New Query, здесь наверху мы должны выбрать нужную нам базу данных. В данном случае у меня есть чуть больше доступов, у вас это симулятор SQL. И вот мы сейчас оказались в симуляторе SQL, у нас тут слева такой слоник нарисован, а вот внизу вы видите уже набор знакомых нам табличек, про которые мы поговорили в прошлом уроке. Теперь пришло время сделать, как говорится, важное объявление. Итак, SQL это просто язык, на котором мы общаемся с базами данных. Он так и переводится – Structured Query Language. То есть некоторый язык, команд, который позволяет нам говорить, что мы хотим из баз данных достать и в каком формате. А база данных – это хранилище наших данных, та коробка, в которую мы кладем нужные нам таблицы. Причем таблицы – это просто как нам удобнее так представлять. Понятное дело, что в самой базе данных данные могут храниться куда более сложным образом. Теперь, что за слоник у нас нарисован? Слоник – это как раз таки база данных, это логотип, который использует PostgreSQL. Вот PostgreSQL – это уже непосредственно база данных, которая хранит в себе нашу информацию про курьеров, пользователей и так далее. И поэтому здесь надо немножко не путать эти вещи. Я понимаю, что ребята, которые уже в курсе, наверное, это все скучно слушать, но те, кто только начинают, давайте еще раз проговорим. PostgreSQL – это хранилище данных. SQL – это язык запросов, при помощи которых мы можем обращаться к нашей базе данных.', '6e4fef38-78bf-4d1c-a230-47ecb1ab4089': 'Что должен уметь человек, который только собирается стать аналитиком данных? В первую очередь он должен знать Python, SQL, Clickhouse, MySQL. Дальше он должен уметь строить дашборды и работать с данными, то есть табло, редаж. Он должен быть уверенным пользователем Windows, Unix, работа с продуктовыми инструментами Microsoft Excel, то есть это различные сводные таблицы, это WPR, потому что некоторые компании, например, больше акцент делают на Excel. И об этом лучше уточнить заранее на субсидиуме работодателя, потому что бывают иногда кейсы, человек хочет, чтобы у него весь стек использовался, Python, SQL и так далее, а он приходит и на рабочем месте у него рабочий инструмент Excel. И ожидания реальности могут не совпадать. Дальше человек должен работать в юбитере, гид, Airflow, знать математическую статистику для анализа данных. И не для всех, но лучше знать, например, продуктовую аналитику, понимание развития продукта. Это если вы собираетесь быть продуктовым аналитиком. Но после обучения, например, здесь у вас будет достаточно широкий пул, то есть вы можете стать как продуктовым аналитиком, вы можете стать аналитиком BI, вы можете иногда стать веб-аналитиком, если у вас был маркетинговый бэкграунд, и одно на другое наслоить, и вы можете быть крутым веб-аналитиком, то здесь уже ваше решение, наверное, что вам больше интересно, в каких компаниях вы хотели бы себя реализовать. Я перечислила сейчас основные требования, то есть это те hard skills, которые определяет рынок труда к аналитику данных начинашки, джуну. Но есть soft skills, что такое soft skills и какие soft skills нужны аналитику данных. То есть в первую очередь это коммуникабельность, потому что вам придется достаточно много общаться, общаться в рамках рабочей группы, в зависимости от того, в какую команду вы попадете, для того, чтобы вам работать с данными, вам нужно понимать, ага, где мне нужно собрать эти данные, что мне нужно получить на выходе, что от меня ждут, кто заказчик.', '8a0d68ec-b438-4309-8631-bf4c5990d3d1': 'Всем привет! С вами Толя Карпов и мы продолжаем говорить про полезные инструменты для анализа данных. Сегодня поговорим про оконные функции и как их можно применять в SQL или в Pandas. Соответственно, сделаем такое небольшое сравнение этого инструмента в зависимости от того, где вы работаете с данными в SQL или в Python. Я думаю, что если вы еще и не применяли оконные функции, то точно про них что-то слышали. Это довольно полезная вещь при анализе данных. Сегодня мы посмотрим, какие задачки можно при помощи оконных решать и как синтаксис оконных функций различается в зависимости от того, где вы с данными работаете. Сегодня мы будем использовать набор данных про онлайн-магазин. Здесь у нас есть табличка User Actions и в ней представлена дата и действия пользователей с различными заказами. Мы отберем только создание заказа и получим вот такой вот набор данных. Я их отсортировал по пользователю и мы видим, что, допустим, пользователь 1 создал за всю историю наших данных 4 заказа, пользователь 2 создал 2 заказа и так далее. Соответственно, первое, что нам хочется сделать, мы видим, что у пользователей количество заказов отличается. Кто-то там создал 4 заказа, кто-то 2, кто-то 7. Мы хотели бы добавить еще одну колонку, которая просто нам показывает номер заказа этого пользователя. Это довольно полезно для аналитики. Мы сможем легко сразу отобрать только первый заказ или довольно быстро после этого посчитать, сколько всего было заказов у человека или сравнить с содержанием первого и последнего заказа и так далее. Как же нам это сделать при помощи Postgres SQL? Для этого нам потребуются оконные функции и специальная функция rank. Вообще, есть довольно много функций, которые как-то агрегируют наши данные. Максимум, среднее, которые можно использовать с грубой. А еще есть такие специальные функции, которые именно используются в контексте оконных функций. Значит, функция rank сама по себе очень простая. Она просто проставляет порядковый номер с некоторой строки, но ей нужно сказать, как именно она должна это делать. То есть, что мы хотим? Мы хотим, чтобы наша функция внутри каждого пользователя отсортировала по времени его товары. Самый ранний товар, соответственно, был бы первый, следующий товар был бы второй, последний товар был бы, грубо говоря, n. И как только мы перешли к новому пользователю, мы снова начали бы нумерацию с единички. Опять, у этого пользователя был бы первый товар, второй и так далее. Значит, мы пишем функцию rank.', 'd85f128d-ef9e-4483-8942-3670cd02351f': 'Ну, здесь вот я в качестве примера написал такие базовые вещи, там просто возраст и пол. Вот это вот наши данные нашего продукта. И понятное дело, что хранить данные в таком виде в гугл-документе не самая хорошая идея. Не только потому, что гугл-документ не самая продвинутая хранилища для данных, но и потому, что если у нас будет сотни пользователей, тысячи заказов, тысячи клиентов, то вот просто пополнять вот такую структуру данных новыми записями, как вы понимаете, не очень удобно. Они довольно легко запутаться, и вообще это не самый удобный способ для работы. Поэтому в индустрии данные хранятся в специальных хранилищах. Эти хранилища называются базы данных. И сейчас мы не будем углубляться в то, какие базы данных бывают, а я скажу, что их бывает очень много, под разные типы задач, под разные данные. Сейчас главное понять основную идею. База данных — это такая коробка в первом смысле слова, куда мы кладем наши данные. И самым простым способом представить, что это действительно для нас, аналитиков, такая вот табличная структура. То есть, допустим, пользователь один сделал такое-то событие в такое-то время, и вот эти данные как бы пошли по строчкам писаться. И как организовать хранение данных в нашей компании — это вот одна из таких очень интересных задач, которые часто решают дата-инженеры, архитекторы, но и аналитики могут в этом помогать. Мы с вами сделали такую первую версию, просто написали вот условно ручкой на бумажке всю информацию про одно событие, про один заказ. Но, как вы понимаете, как я сказал, хранить много данных в таком формате не очень удобно. И поэтому на практике используется различный набор баз данных, которые хранят, каждая база данных, какую-то свою часть информации. Эти базы данных между собой как-то связаны, и в целом они как бы покрывают хранение всех данных нашего продукта. Вот здесь на слайде вы видите то, как устроено хранение наших данных в нашем продукте. То есть, ваш первый работчик в нашей компании начинается с того, что я сейчас расскажу вообще, что за данные у нас хранятся, и как мы с ними дальше будем работать, перед тем, как мы уже перейдем к каким-то там базовым искольдзапросам. Вот в прошлый раз мы посмотрели, что как бы у нас есть некоторое такое вот end-to-end событие. Пользователь один заказал и курьер один доставил.'})\n",
      "('relevant_docs', {'431ae521-f3cc-4508-8d1e-d4cca770c01d': ['42682ca2-1051-4d31-8964-7e941bbaca28'], '983cc25f-494b-4e4a-9743-ffe977a71aaf': ['3ef1626e-080e-44f0-a84e-d5d023c0c5d6'], '3a18b2a8-a432-4104-9254-a7ccce061fde': ['0e9b7684-d754-49d4-b3a9-59bb55a62723'], 'caa9b9f5-615b-49e0-ac22-e633280e97d0': ['f8173775-a0ce-42c1-adb5-7682c4e2fb68'], '020cd976-1670-43c5-99cc-2a3f10295841': ['1907c3bc-201f-4dbf-876e-106db1911278'], '3faea57e-9f5c-453b-8e86-a2c1b0913ffd': ['95a28af4-63f2-4d95-a90c-6795746569bf'], 'd54de249-9956-4f9a-9d50-d478fb9792f0': ['6e76feb9-af66-4c7b-bdc9-f26666c0d063'], '78a445b7-d793-41f6-b884-1a6467be8080': ['6e4fef38-78bf-4d1c-a230-47ecb1ab4089'], 'a945cb2e-feb0-44c3-8b5f-3457f74b450b': ['8a0d68ec-b438-4309-8631-bf4c5990d3d1'], '81f5cbd3-f999-450b-9300-7f5d9dfee352': ['d85f128d-ef9e-4483-8942-3670cd02351f']})\n",
      "('mode', 'text')\n"
     ]
    }
   ],
   "source": [
    "for q in qa_dataset:\n",
    "    print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpovai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
