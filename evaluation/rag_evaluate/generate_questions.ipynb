{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node sampler - sample more relevant (informative) nodes \n",
    "# qagenerator - generate questions to the sampled nodes\n",
    "from evaluate_rag_utils import NodeSampler, QAGenerator, Validator\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.finetuning import EmbeddingQAFinetuneDataset\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# make the async code working in the notebook cells\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.evaluation import DatasetGenerator, RelevancyEvaluator, FaithfulnessEvaluator\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import ServiceContext\n",
    "from openai import AsyncOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load question: node dataset\n",
    "qa_dataset_1000 = EmbeddingQAFinetuneDataset.from_json('question_node_dataset_1000.json')\n",
    "# get the questions\n",
    "eval_questions = qa_dataset_1000.queries.values()\n",
    "\n",
    "# load indexed nodes\n",
    "storage_cntxt_512 = StorageContext.from_defaults(persist_dir=\"../../data/index_storage_512\")\n",
    "idx_512 = load_index_from_storage(storage_cntxt_512)\n",
    "storage_cntxt_1024 = StorageContext.from_defaults(persist_dir=\"../../data/index_storage_1024\")\n",
    "idx_1024 = load_index_from_storage(storage_cntxt_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len information = 3\n",
      "len information = 14\n",
      "len information = 3\n",
      "len information = 7\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    client = AsyncOpenAI()\n",
    "    semaphore = asyncio.Semaphore(100)\n",
    "    params_list = [\n",
    "        {\n",
    "            'client': client,\n",
    "            'index': idx_1024,\n",
    "            'semaphore': semaphore\n",
    "            },\n",
    "        {\n",
    "            'client': client,\n",
    "            'index': idx_512,\n",
    "            'semaphore': semaphore,\n",
    "            'has_node_postprocessors': True\n",
    "            'num_nodes': 1\n",
    "            }\n",
    "    ]\n",
    "    # classes that validate QA dataset with default evaluators\n",
    "    validators = [Validator(**params) for params in params_list]\n",
    "\n",
    "    output_paths = [\n",
    "        'rag_evaluate_json_1024.json',\n",
    "        'rag_evaluate_json_512_postprocess_both_1.json',\n",
    "    ]\n",
    "\n",
    "    # generate Q-A for 2 questions and validate them\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "                    tasks = [\n",
    "                        tg.create_task(\n",
    "                            validator.answers_evaluated_list(\n",
    "                                queries=list(eval_questions)[:2],\n",
    "                                output_path=output_path\n",
    "                                )\n",
    "                        )\n",
    "                        for validator, output_path in zip(validators, output_paths)\n",
    "                    ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more relevant nodes\n",
    "# having higher counts of relevant and lower count of irrelevant words\n",
    "# limit sample to 5% of all nodes\n",
    "node_sampler = NodeSampler('relevant_words.txt', 'irrelevant_words.txt')\n",
    "informative_nodes_1000 = node_sampler.informative_nodes(\n",
    "    video_info_path='../../data/video_info.json', \n",
    "    chunk_size=1024, \n",
    "    fraction=0.05\n",
    "    )\n",
    "\n",
    "informative_nodes_3000 = node_sampler.informative_nodes(\n",
    "    video_info_path='../../data/video_info.json', \n",
    "    chunk_size=1024*3, \n",
    "    fraction=0.05\n",
    "    )\n",
    "\n",
    "\n",
    "nodes_1000 = informative_nodes_1000[:10]\n",
    "nodes_3000 = informative_nodes_3000[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# create QA generator\n",
    "qa_generator = QAGenerator(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate:\n",
    "# i) question-node pairs, save as json at qa_json_path\n",
    "# ii) question-document pairs at doc_question_path\n",
    "qa_dataset_1000 = qa_generator.generate_document_question_pairs(\n",
    "    nodes=nodes_1000,\n",
    "    doc_json_path='../../data/video_info.json',\n",
    "    doc_question_path='doc_question_dataset_1000.json',\n",
    "    qa_json_path='question_node_dataset_1000.json'\n",
    ")\n",
    "\n",
    "qa_dataset_3000 = qa_generator.generate_document_question_pairs(\n",
    "    nodes=nodes_3000,\n",
    "    doc_json_path='../../data/video_info.json',\n",
    "    doc_question_path='doc_question_dataset_3000.json',\n",
    "    qa_json_path='question_node_dataset_3000.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_1000 = EmbeddingQAFinetuneDataset.from_json('question_node_dataset_1000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['\"Как работает алгоритм градиентного бустинга в анализе данных?\"', '\"Какие основные компетенции должен иметь аналитик данных?\"', '\"Что такое атрибуты и методы в Pandas DataFrame?\"', '\"Как дерево решений выбирает вопрос для прогнозирования данных?\"', 'Какие навыки должен иметь аналитик данных согласно тексту?', '\"Какие изменения были внесены в запрос для анализа данных?\"', 'Какие хардскиллы важны для аналитика данных в сфере программирования?', 'Какие инструменты аналитики рекомендуются для работы с данными?', 'Как работает метод изоляционного леса для обнаружения выбросов?', '\"Какие навыки должен иметь аналитик данных согласно тексту?\"'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_1000.queries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is loaded\n"
     ]
    }
   ],
   "source": [
    "storage_cntxt = StorageContext.from_defaults(persist_dir=\"../../data/index_storage_1024\")\n",
    "idx = load_index_from_storage(storage_cntxt)\n",
    "print(\"Index is loaded\")\n",
    "\n",
    "eval_questions = qa_dataset_1000.queries.values()\n",
    "\n",
    "\n",
    "# service_context_gpt3 = ServiceContext.from_defaults(llm=llm)\n",
    "# rel_evaluator_gpt3 = RelevancyEvaluator(service_context=service_context_gpt3)\n",
    "# faith_evaluator_gpt3 = FaithfulnessEvaluator(service_context=service_context_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = Validator(\n",
    "    client=client,\n",
    "    index=idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = (\n",
    "    list(qa_dataset_1000.queries.values())[0]\n",
    ")\n",
    "query_engine = idx.as_query_engine()\n",
    "response_vector = query_engine.query(query_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = rel_evaluator_gpt3.evaluate_response(\n",
    "    query=query_str, response=response_vector\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jupyter display function\n",
    "def display_eval_df(query: str, response, eval_result: str) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a8484_row0_col1, #T_a8484_row0_col2 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a8484\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a8484_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_a8484_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_a8484_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
       "      <th id=\"T_a8484_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a8484_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a8484_row0_col0\" class=\"data row0 col0\" >\"Как работает алгоритм градиентного бустинга в анализе данных?\"</td>\n",
       "      <td id=\"T_a8484_row0_col1\" class=\"data row0 col1\" >Алгоритм градиентного бустинга в анализе данных работает путем комбинирования множества деревьев решений для улучшения точности прогнозирования. Каждое новое дерево решений является самостоятельным алгоритмом машинного обучения, который использует те же данные, что и предыдущие деревья. Каждое дерево способно давать прогноз самостоятельно. Градиентный бустинг использует градиентный спуск для постепенного улучшения прогноза, минимизируя ошибку между прогнозами и фактическими значениями. Этот процесс повторяется до достижения определенного критерия остановки, такого как достижение определенного числа деревьев или улучшение прогноза не наблюдается.</td>\n",
       "      <td id=\"T_a8484_row0_col2\" class=\"data row0 col2\" >Ребята, всем привет! Меня зовут Григорий Бударагин, я аналитик-разработчик компании Яндекс. Сегодня я расскажу вам механизм работы градиентного бустинга. Что же такое градиентный бустинг? Это алгоритм машинного обучения, который помогает прогнозировать различные числовые величины. Например, он используется в различных больших технологических компаниях для ранжирования веб-страниц, для выставления цен в приложении такси, а также для противодействия мошенничеству и многого другого. Не только градиентный бустинг, но и другие алгоритмы машинного обучения учатся прогнозировать числовые величины на каких-то исторических наблюдениях. Например, чтобы научить алгоритм предсказывать цену дома, ему необходимо наблюдение о проданных в прошлом домах с их ценами и другими измеряемыми характеристиками....</td>\n",
       "      <td id=\"T_a8484_row0_col3\" class=\"data row0 col3\" >Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1334e1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_eval_df(query_str, response_vector, eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_llm_answer(\n",
    "    query_engine,\n",
    "    query: str,\n",
    "    context_prompt: str = None,\n",
    "    model_name: str = \"gpt-3.5-turbo-1106\",\n",
    ") -> str:\n",
    "\n",
    "    retrival = await query_engine.aquery(message)\n",
    "    information = [\n",
    "        (i.text, i.metadata[\"url\"], i.metadata[\"title\"]) for i in retrival.source_nodes\n",
    "    ]\n",
    "\n",
    "    for text in [text for text, _, _ in information]:\n",
    "        logging.info(text)\n",
    "\n",
    "    information_text = escape_html(\" \".join([text for text, _, _ in information]))\n",
    "    information_url = \"\\n\".join(\n",
    "        set(\n",
    "            f'&#x25CF; <a href=\"{url}\">{escape_html(title)}</a>'\n",
    "            for _, url, title in information\n",
    "        )\n",
    "    )\n",
    "\n",
    "    context_prompt = (\n",
    "        \"Ниже мы предоставили контекстную информацию\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        f\"{information_text}\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        f\"Учитывая эту информацию, ответьте, пожалуйста, на вопрос: {message}\\n\"\n",
    "        \"\\n---------------------\\n\"\n",
    "        \"Ответ на вопрос должен быть развернутым, полным, и охватывать множество \"\n",
    "        \"аспектов заданного вопроса\"\n",
    "        \"Внимание! В ответе нельзя упоминать конекстную информацию! \"\n",
    "        \"Пользователь не знает о ее наличии!\"\n",
    "    )\n",
    "\n",
    "    logging.info(\"Сообщение сформировано и отправлено в OpenAI\")\n",
    "    model_name = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    context_response = await client.chat.completions.create(\n",
    "        model=model_name, temperature=0, messages=[{\"role\": \"user\", \"content\": context_prompt}]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab{c}_{d}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'ab{c}_{d}'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab4_1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.format_map({'c': 4, 'd': 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpovai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
